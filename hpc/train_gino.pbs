#!/bin/bash
#PBS -N GINO
#PBS -l select=1:ncpus=4:mem=46gb:ngpus=1
#PBS -l walltime=12:00:00
#PBS -m abe
#PBS -j oe
#PBS -o /srv/scratch/z5370003/projects/src/04_groundwater/variable_density/logs/train_gino.log

# USAGE:
# Directory structure: RESULTS_BASE_DIR/TARGET_COL/EXPERIMENT_NAME/gino_TIMESTAMP/
# 
# EXPERIMENT MANAGEMENT:
# Experiments are auto-named based on hyperparameters (e.g., exp_lr5e4_cos_bs128)
# Each experiment maintains independent checkpoints and can be resumed separately
# 
# BASIC USAGE:
# Resume default experiment:         qsub train_gino.pbs
# Start fresh experiment:            qsub -v RESUME_TRAINING=false train_gino.pbs
# Train different target:            qsub -v TARGET_COL=head train_gino.pbs
# 
# HYPERPARAMETER EXPERIMENTS:
# Different learning rates:          qsub -v LEARNING_RATE=1e-3 train_gino.pbs
# Different schedulers:              qsub -v SCHEDULER_TYPE=exponential train_gino.pbs
# Different batch sizes:             qsub -v BATCH_SIZE=256 train_gino.pbs
# Combined changes:                  qsub -v "LEARNING_RATE=2e-4,BATCH_SIZE=64,GRAD_CLIP_NORM=2.0" train_gino.pbs
# 
# MANUAL EXPERIMENT NAMING:
# Custom experiment name:            qsub -v EXPERIMENT_NAME=my_custom_experiment train_gino.pbs
# Resume specific experiment:        qsub -v "EXPERIMENT_NAME=exp_lr1e3_exp_bs256" train_gino.pbs
# 
# EXAMPLES:
# - Default (lr=5e-4, cosine, bs=128): RESULTS_BASE_DIR/mass_concentration/exp_lr5e4_cos_bs128/
# - High LR experiment:               RESULTS_BASE_DIR/mass_concentration/exp_lr1e3_cos_bs128/
# - Large batch experiment:           RESULTS_BASE_DIR/mass_concentration/exp_lr5e4_cos_bs256/
# - Custom gradient clipping:         RESULTS_BASE_DIR/mass_concentration/exp_lr5e4_cos_bs128_clip2/
# 
# You can also edit the variables below to change the default behavior


# Change to the directory from which the job was submitted
cd $PBS_O_WORKDIR/..

# Set up variables
RESULTS_BASE_DIR="${RESULTS_BASE_DIR:-/srv/scratch/z5370003/projects/results/04_groundwater/variable_density/GINO/}"
PYTHON_ENV="/srv/scratch/z5370003/miniconda3/envs/torch-env/bin/python"
BASE_DATA_DIR="/srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/"

# Training parameters (improved for better convergence)
# These can be overridden from command line using qsub -v
EPOCHS="${EPOCHS:-40}"
BATCH_SIZE="${BATCH_SIZE:-128}"
CHECKPOINT_EVERY="${CHECKPOINT_EVERY:-10}"
INPUT_WINDOW="${INPUT_WINDOW:-5}"
OUTPUT_WINDOW="${OUTPUT_WINDOW:-5}"
TARGET_COL="${TARGET_COL:-mass_concentration}"

# Improved hyperparameters for better convergence
# These can also be overridden from command line
LEARNING_RATE="${LEARNING_RATE:-5e-4}"
LR_GAMMA="${LR_GAMMA:-0.98}"
LR_SCHEDULER_INTERVAL="${LR_SCHEDULER_INTERVAL:-10}"
GRAD_CLIP_NORM="${GRAD_CLIP_NORM:-1.0}"
SCHEDULER_TYPE="${SCHEDULER_TYPE:-cosine}"

# Control resuming behavior - set to "true" to resume from checkpoint, "false" to start fresh
# This can be overridden from command line: qsub -v RESUME_TRAINING=false train_gino.pbs
RESUME_TRAINING="${RESUME_TRAINING:-true}"

# Experiment naming and management
# EXPERIMENT_NAME can be set manually or auto-generated from hyperparameters
EXPERIMENT_NAME="${EXPERIMENT_NAME:-}"

# Function to generate experiment name from hyperparameters
generate_experiment_name() {
    local lr_clean=$(echo "$LEARNING_RATE" | sed 's/e-0*/e/')  # Clean up scientific notation
    local scheduler_short="${SCHEDULER_TYPE:0:3}"  # First 3 letters (cos/exp)
    local name="exp_lr${lr_clean}_${scheduler_short}_bs${BATCH_SIZE}"
    
    # Add gradient clipping if not default
    if [ "$GRAD_CLIP_NORM" != "1.0" ]; then
        local clip_clean=$(echo "$GRAD_CLIP_NORM" | sed 's/\.0$//')
        name="${name}_clip${clip_clean}"
    fi
    
    # Add LR gamma if not default (for exponential scheduler)
    if [ "$SCHEDULER_TYPE" = "exponential" ] && [ "$LR_GAMMA" != "0.98" ]; then
        local gamma_clean=$(echo "$LR_GAMMA" | sed 's/0\.//')
        name="${name}_g${gamma_clean}"
    fi
    
    echo "$name"
}

# Set experiment name (auto-generate if not provided)
if [ -z "$EXPERIMENT_NAME" ]; then
    EXPERIMENT_NAME=$(generate_experiment_name)
fi

# Create experiment-specific results directory structure
TARGET_RESULTS_DIR="$RESULTS_BASE_DIR/$TARGET_COL"
EXPERIMENT_DIR="$TARGET_RESULTS_DIR/$EXPERIMENT_NAME"
echo "Target: $TARGET_COL"
echo "Experiment: $EXPERIMENT_NAME"
echo "Experiment directory: $EXPERIMENT_DIR"

# Check for existing latest checkpoint to resume from (only if RESUME_TRAINING is true)
LATEST_CHECKPOINT=""
if [ "$RESUME_TRAINING" = "true" ]; then
    if [ -d "$EXPERIMENT_DIR" ]; then
        # Find the most recent training run within this experiment
        LATEST_RESULTS_DIR=$(find "$EXPERIMENT_DIR" -maxdepth 1 -type d -name "gino_*" | sort | tail -1)
        
        if [ -n "$LATEST_RESULTS_DIR" ] && [ -f "$LATEST_RESULTS_DIR/checkpoints/latest_checkpoint.pth" ]; then
            LATEST_CHECKPOINT="$LATEST_RESULTS_DIR/checkpoints/latest_checkpoint.pth"
            echo "Found existing checkpoint for experiment '$EXPERIMENT_NAME': $LATEST_CHECKPOINT"
            echo "Resuming training from checkpoint..."
        else
            echo "RESUME_TRAINING=true but no existing checkpoint found for experiment '$EXPERIMENT_NAME'. Starting new training..."
        fi
    else
        echo "RESUME_TRAINING=true but experiment directory '$EXPERIMENT_NAME' does not exist. Starting new experiment..."
        # List available experiments for this target
        if [ -d "$TARGET_RESULTS_DIR" ]; then
            echo "Available experiments for target '$TARGET_COL':"
            find "$TARGET_RESULTS_DIR" -maxdepth 1 -type d -name "exp_*" | sed 's|.*/||' | sort
        fi
    fi
else
    echo "RESUME_TRAINING=false. Starting fresh training for experiment '$EXPERIMENT_NAME'..."
fi

# Build the training command with improved hyperparameters
TRAIN_CMD="$PYTHON_ENV train_gino_on_patches.py \
    --epochs $EPOCHS \
    --batch-size $BATCH_SIZE \
    --base-data-dir $BASE_DATA_DIR \
    --patch-data-subdir filter_patch \
    --input-window-size $INPUT_WINDOW \
    --output-window-size $OUTPUT_WINDOW \
    --target-col $TARGET_COL \
    --save-checkpoint-every $CHECKPOINT_EVERY \
    --learning-rate $LEARNING_RATE \
    --lr-gamma $LR_GAMMA \
    --lr-scheduler-interval $LR_SCHEDULER_INTERVAL \
    --grad-clip-norm $GRAD_CLIP_NORM \
    --scheduler-type $SCHEDULER_TYPE \
    --results-dir $EXPERIMENT_DIR"

# Add resume parameter if checkpoint exists
if [ -n "$LATEST_CHECKPOINT" ]; then
    TRAIN_CMD="$TRAIN_CMD --resume-from $LATEST_CHECKPOINT"
fi

# Log the command being executed
echo "Executing command:"
echo "$TRAIN_CMD"
echo "Starting time: $(date)"

# Run the training script
eval $TRAIN_CMD

echo "Training completed at: $(date)"
