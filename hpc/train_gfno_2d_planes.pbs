#!/bin/bash
#PBS -N GFNO_2DPlanes
#PBS -l select=1:ncpus=2:mem=8gb:ngpus=2:gpu_model=A100
#PBS -l walltime=12:00:00
#PBS -m abe
#PBS -j oe
#PBS -o /srv/scratch/z5370003/projects/src/04_groundwater/variable_density/logs/train_gfno_2d_planes.log

# USAGE:
# Directory structure: RESULTS_BASE_DIR/gfno_2d_planes/EXPERIMENT_NAME/gfno_TIMESTAMP/
# 
# EXPERIMENT MANAGEMENT:
# Experiments are auto-named based on hyperparameters (e.g., exp_lr1e3_cos_bs128)
# Each experiment maintains independent checkpoints and can be resumed separately
# 
# BASIC USAGE:
# Resume default experiment:         qsub train_gfno_2d_planes.pbs
# Start fresh experiment:            qsub -v RESUME_TRAINING=false train_gfno_2d_planes.pbs
# 
# HYPERPARAMETER EXPERIMENTS:
# Different learning rates:          qsub -v LEARNING_RATE=5e-4 train_gfno_2d_planes.pbs
# Different schedulers:              qsub -v SCHEDULER_TYPE=exponential train_gfno_2d_planes.pbs
# Different batch sizes:             qsub -v BATCH_SIZE=256 train_gfno_2d_planes.pbs
# Combined changes:                  qsub -v "LEARNING_RATE=2e-4,BATCH_SIZE=64,GRAD_CLIP_NORM=2.0" train_gfno_2d_planes.pbs
# 
# MANUAL EXPERIMENT NAMING:
# Custom experiment name:            qsub -v EXPERIMENT_NAME=my_custom_experiment train_gfno_2d_planes.pbs
# Resume specific experiment:        qsub -v "EXPERIMENT_NAME=exp_lr1e3_exp_bs256" train_gfno_2d_planes.pbs
# 
# EXAMPLES:
# - Default (lr=1e-3, cosine, bs=128): 
#   RESULTS_BASE_DIR/gfno_2d_planes/exp_lr1e3_cos_bs128/
# - High LR experiment:               
#   RESULTS_BASE_DIR/gfno_2d_planes/exp_lr5e4_cos_bs128/
# - Large batch experiment:           
#   RESULTS_BASE_DIR/gfno_2d_planes/exp_lr1e3_cos_bs256/
# 
# You can also edit the variables below to change the default behavior


# Change to the directory from which the job was submitted
cd $PBS_O_WORKDIR/..

# Set up variables
RESULTS_BASE_DIR="${RESULTS_BASE_DIR:-/srv/scratch/z5370003/projects/results/04_groundwater/2d_planes/GFNO/}"
PYTHON_ENV="/srv/scratch/z5370003/miniconda3/envs/torch-env/bin/python"
DATA_DIR="/srv/scratch/z5370003/projects/data/GW/2d_plane_sequences"

# Training parameters (improved for better convergence)
# These can be overridden from command line using qsub -v
EPOCHS="${EPOCHS:-100}"
BATCH_SIZE="${BATCH_SIZE:-128}"
CHECKPOINT_EVERY="${CHECKPOINT_EVERY:-10}"

# Improved hyperparameters for better convergence
# These can also be overridden from command line
LEARNING_RATE="${LEARNING_RATE:-1e-3}"
LR_GAMMA="${LR_GAMMA:-0.98}"
LR_SCHEDULER_INTERVAL="${LR_SCHEDULER_INTERVAL:-10}"
GRAD_CLIP_NORM="${GRAD_CLIP_NORM:-1.0}"
SCHEDULER_TYPE="${SCHEDULER_TYPE:-cosine}"

# Control resuming behavior - set to "true" to resume from checkpoint, "false" to start fresh
# This can be overridden from command line: qsub -v RESUME_TRAINING=false train_gfno_2d_planes.pbs
RESUME_TRAINING="${RESUME_TRAINING:-true}"

# Run selection for resuming training
# RUN_NAME can be set to resume from a specific run (e.g., gfno_20251103_153951)
# If not set, uses the most recent run with a valid checkpoint
RUN_NAME="${RUN_NAME:-}"

# Experiment naming and management
# EXPERIMENT_NAME can be set manually or auto-generated from hyperparameters
EXPERIMENT_NAME="${EXPERIMENT_NAME:-}"

# Function to generate experiment name from hyperparameters
generate_experiment_name() {
    local lr_clean=$(echo "$LEARNING_RATE" | sed 's/e-0*/e/')  # Clean up scientific notation
    local scheduler_short="${SCHEDULER_TYPE:0:3}"  # First 3 letters (cos/exp)
    local name="exp_lr${lr_clean}_${scheduler_short}_bs${BATCH_SIZE}"
    
    # Add gradient clipping if not default
    if [ "$GRAD_CLIP_NORM" != "1.0" ]; then
        local clip_clean=$(echo "$GRAD_CLIP_NORM" | sed 's/\.0$//')
        name="${name}_clip${clip_clean}"
    fi
    
    # Add LR gamma if not default (for exponential scheduler)
    if [ "$SCHEDULER_TYPE" = "exponential" ] && [ "$LR_GAMMA" != "0.98" ]; then
        local gamma_clean=$(echo "$LR_GAMMA" | sed 's/0\.//')
        name="${name}_g${gamma_clean}"
    fi
    
    echo "$name"
}

# Set experiment name (auto-generate if not provided)
if [ -z "$EXPERIMENT_NAME" ]; then
    EXPERIMENT_NAME=$(generate_experiment_name)
fi

# Create experiment-specific results directory structure
# Structure: RESULTS_BASE_DIR/gfno_2d_planes/EXPERIMENT_NAME/
GFNO_RESULTS_DIR="$RESULTS_BASE_DIR/gfno_2d_planes"
EXPERIMENT_DIR="$GFNO_RESULTS_DIR/$EXPERIMENT_NAME"

echo "=========================================="
echo "GFNO 2D Planes Training Configuration"
echo "=========================================="
echo "Experiment: $EXPERIMENT_NAME"
echo "Experiment directory: $EXPERIMENT_DIR"
echo ""
echo "Hyperparameters:"
echo "  - Learning rate: $LEARNING_RATE"
echo "  - Scheduler: $SCHEDULER_TYPE"
echo "  - Batch size: $BATCH_SIZE"
echo "  - Gradient clip norm: $GRAD_CLIP_NORM"
echo "  - Epochs: $EPOCHS"
echo "=========================================="

# Function to list available runs for an experiment
list_available_runs() {
    local exp_dir="$1"
    if [ -d "$exp_dir" ]; then
        echo "Available runs for experiment '$EXPERIMENT_NAME':"
        echo "----------------------------------------"
        for run_dir in $(find "$exp_dir" -maxdepth 1 -type d -name "gfno_*" | sort); do
            local run_name=$(basename "$run_dir")
            local checkpoint_path="$run_dir/checkpoints/latest_checkpoint.pth"
            local has_checkpoint="No"
            local checkpoint_time=""
            if [ -f "$checkpoint_path" ]; then
                has_checkpoint="Yes"
                checkpoint_time=$(date -r "$checkpoint_path" "+%Y-%m-%d %H:%M:%S")
            fi
            echo "Run: $run_name"
            echo "  - Path: $run_dir"
            echo "  - Has checkpoint: $has_checkpoint"
            if [ -n "$checkpoint_time" ]; then
                echo "  - Last checkpoint: $checkpoint_time"
            fi
            echo "----------------------------------------"
        done
    fi
}

# Check for existing checkpoint to resume from (only if RESUME_TRAINING is true)
LATEST_CHECKPOINT=""
SELECTED_RUN_DIR=""
if [ "$RESUME_TRAINING" = "true" ]; then
    if [ -d "$EXPERIMENT_DIR" ]; then
        # List all available runs
        list_available_runs "$EXPERIMENT_DIR"
        
        if [ -n "$RUN_NAME" ]; then
            # Use specified run if RUN_NAME is provided
            SELECTED_RUN_DIR="$EXPERIMENT_DIR/$RUN_NAME"
            if [ ! -d "$SELECTED_RUN_DIR" ]; then
                echo "Error: Specified run '$RUN_NAME' not found in experiment '$EXPERIMENT_NAME'"
                exit 1
            fi
            echo "Using specified run: $RUN_NAME"
        else
            # Find the most recent training run within this experiment
            SELECTED_RUN_DIR=$(find "$EXPERIMENT_DIR" -maxdepth 1 -type d -name "gfno_*" | sort | tail -1)
            if [ -n "$SELECTED_RUN_DIR" ]; then
                echo "Using most recent run: $(basename "$SELECTED_RUN_DIR")"
            fi
        fi
        
        # Check for checkpoint in selected run
        if [ -n "$SELECTED_RUN_DIR" ]; then
            CHECKPOINT_PATH="$SELECTED_RUN_DIR/checkpoints/latest_checkpoint.pth"
            if [ -f "$CHECKPOINT_PATH" ]; then
                LATEST_CHECKPOINT="$CHECKPOINT_PATH"
                echo "Found checkpoint: $LATEST_CHECKPOINT"
                echo "Last modified: $(date -r "$LATEST_CHECKPOINT" "+%Y-%m-%d %H:%M:%S")"
                echo "Resuming training from checkpoint..."
            else
                echo "No checkpoint found in selected run. Starting new training..."
            fi
        fi
    else
        echo "RESUME_TRAINING=true but experiment directory '$EXPERIMENT_NAME' does not exist. Starting new experiment..."
        # List available experiments
        if [ -d "$GFNO_RESULTS_DIR" ]; then
            echo "Available experiments:"
            find "$GFNO_RESULTS_DIR" -maxdepth 1 -type d -name "exp_*" | sed 's|.*/||' | sort
        fi
    fi
else
    echo "RESUME_TRAINING=false. Starting fresh training for experiment '$EXPERIMENT_NAME'..."
fi

# Use selected run directory for resuming
LATEST_RESULTS_DIR="$SELECTED_RUN_DIR"

# Build the training command with improved hyperparameters
TRAIN_CMD="$PYTHON_ENV train_gfno_2d_planes.py \
    --data-dir $DATA_DIR \
    --results-dir $EXPERIMENT_DIR \
    --epochs $EPOCHS \
    --batch-size $BATCH_SIZE \
    --save-checkpoint-every $CHECKPOINT_EVERY \
    --learning-rate $LEARNING_RATE \
    --lr-gamma $LR_GAMMA \
    --lr-scheduler-interval $LR_SCHEDULER_INTERVAL \
    --grad-clip-norm $GRAD_CLIP_NORM \
    --scheduler-type $SCHEDULER_TYPE \
    --device cuda \
    --use-multi-gpu \
    --seed 42"

# Add resume parameter if checkpoint exists and use the existing results directory
if [ -n "$LATEST_CHECKPOINT" ]; then
    if [ -n "$LATEST_RESULTS_DIR" ]; then
        TRAIN_CMD="$TRAIN_CMD --resume-from $LATEST_CHECKPOINT --results-dir $LATEST_RESULTS_DIR"
        echo "Resuming training in existing directory: $LATEST_RESULTS_DIR"
    else
        TRAIN_CMD="$TRAIN_CMD --resume-from $LATEST_CHECKPOINT"
        echo "Warning: Could not find existing results directory, will create new one"
    fi
fi

# Log the command being executed
echo ""
echo "=========================================="
echo "Executing command:"
echo "$TRAIN_CMD"
echo "=========================================="
echo "Starting time: $(date)"
echo ""

# Run the training script
eval $TRAIN_CMD

echo ""
echo "=========================================="
echo "Training completed at: $(date)"
echo "Results saved to: $EXPERIMENT_DIR"
echo "=========================================="
