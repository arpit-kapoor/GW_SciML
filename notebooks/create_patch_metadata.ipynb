{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Patched Groundwater Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plot the patches in 3D using plotly for interactivity\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base data directory: /srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/\n",
      "Filtered data directory: /srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/filter\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define data directories\n",
    "base_data_dir = '/srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/'\n",
    "# base_data_dir = '/Users/arpitkapoor/Library/CloudStorage/OneDrive-UNSW/Shared/Projects/01_PhD/05_groundwater/data/FEFLOW/variable_density'  # Uncomment for local testing\n",
    "raw_data_dir = os.path.join(base_data_dir, 'all')\n",
    "filtered_data_dir = os.path.join(base_data_dir, 'filter')\n",
    "\n",
    "print(f\"Base data directory: {base_data_dir}\")\n",
    "print(f\"Filtered data directory: {filtered_data_dir}\")\n",
    "print(f\"Directory exists: {os.path.exists(filtered_data_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Sort Time Series Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 1909\n",
      "First 3 files: ['0000.csv', '0001.csv', '0002.csv']\n",
      "Last 3 files: ['1906.csv', '1907.csv', '1908.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get and sort time series files\n",
    "ts_files = sorted(os.listdir(raw_data_dir))\n",
    "print(f\"Total number of files: {len(ts_files)}\")\n",
    "print(f\"First 3 files: {ts_files[:3]}\")\n",
    "print(f\"Last 3 files: {ts_files[-3:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Patches for Grid and Function Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def create_patches(x, y, z, \n",
    "                   slice_id, n_patches, \n",
    "                   slice_split=3, \n",
    "                   ghost_points_ratio=0.2,\n",
    "                   n_ghost_points=None):\n",
    "    \"\"\"\n",
    "    Assigns a patch id to each point such that each patch has (almost) the same number of points.\n",
    "    Also identifies neighbouring patches and finds the closest ghost points from neighbouring patches to each patch.\n",
    "\n",
    "    This function uses K-means clustering to divide 3D points into patches, where points are first grouped\n",
    "    by slice, then clustered within each slice group. It also identifies patch neighbors and selects\n",
    "    ghost points from neighboring patches for boundary conditions.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): X coordinates of points.\n",
    "        y (array-like): Y coordinates of points.\n",
    "        z (array-like): Z coordinates of points.\n",
    "        slice_id (array-like): Slice/group identifier for each point.\n",
    "        n_patches (int): Total number of patches (clusters) to create across all slice groups.\n",
    "        slice_split (int): Number of slice groups to divide the data into. Default is 3.\n",
    "        ghost_points_ratio (float): Ratio of patch size to determine number of ghost points when \n",
    "                                  n_ghost_points is None. Default is 0.2.\n",
    "        n_ghost_points (int, optional): Fixed number of ghost points to find from each neighbor.\n",
    "                                       If None, calculated as ghost_points_ratio * patch_size.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - patch_ids (np.ndarray): Array of patch ids assigned to each point.\n",
    "            - slice_groups (np.ndarray): Array of slice group ids for each point.\n",
    "            - patch_neighbours (dict): Mapping from patch id to set of neighbouring patch ids.\n",
    "            - patch_ghost_points (dict): Mapping from patch id to dict of \n",
    "                                       {neighbour_patch_id: ghost_point_indices}.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the slices into slice_split groups for better spatial distribution\n",
    "    slices = np.sort(np.unique(slice_id))\n",
    "    slices_split = np.array_split(slices, slice_split)\n",
    "\n",
    "    # Calculate the number of patches per slice group\n",
    "    n_patches_per_slice = n_patches // slice_split\n",
    "\n",
    "    # Initialize arrays to store patch assignments and slice group memberships\n",
    "    patch_ids = np.empty(x.shape[0], dtype=int)\n",
    "    slice_groups = np.empty(x.shape[0], dtype=int)\n",
    "\n",
    "    patch_id_offset = 1  # Start patch ids from 1 for better readability\n",
    "\n",
    "    # Process each slice group separately\n",
    "    for i, slice_group in enumerate(slices_split):\n",
    "        print(f\"Processing slice group {i+1}: {slice_group}\")\n",
    "\n",
    "        # Get indices of points belonging to current slice group\n",
    "        slice_idx = np.where(np.isin(slice_id, slice_group))[0]\n",
    "        slice_groups[slice_idx] = i+1\n",
    "\n",
    "        # Extract coordinates for points in current slice group\n",
    "        x_slice = x[slice_idx]\n",
    "        y_slice = y[slice_idx]\n",
    "        z_slice = z[slice_idx]\n",
    "\n",
    "        n_points = x_slice.shape[0]\n",
    "\n",
    "        # Stack coordinates for K-means clustering\n",
    "        coords = np.stack([x_slice, y_slice, z_slice], axis=1)\n",
    "\n",
    "        # Handle case where there are fewer points than desired clusters\n",
    "        n_clusters = min(n_patches_per_slice, n_points)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(coords)\n",
    "\n",
    "        # Map local cluster labels to global patch ids\n",
    "        patch_ids_slice = cluster_labels + patch_id_offset\n",
    "        patch_ids[slice_idx] = patch_ids_slice\n",
    "\n",
    "        # Update offset for next slice group\n",
    "        patch_id_offset += n_patches_per_slice\n",
    "\n",
    "    # Identify neighboring patches using k-nearest neighbor approach\n",
    "    # Two patches are considered neighbors if their points are close to each other\n",
    "    coords_all = np.stack([x, y, z], axis=1)\n",
    "    unique_patches = np.unique(patch_ids)\n",
    "    patch_indices = {pid: np.where(patch_ids == pid)[0] for pid in unique_patches}\n",
    "\n",
    "    # Build KDTree for efficient spatial queries\n",
    "    tree = cKDTree(coords_all)\n",
    "\n",
    "    # Find neighbors for each patch\n",
    "    patch_neighbours = defaultdict(set)\n",
    "    for pid in unique_patches:\n",
    "        idx = patch_indices[pid]\n",
    "        # For each point in the patch, find its 10 nearest neighbors\n",
    "        dists, nbrs = tree.query(coords_all[idx], k=10)\n",
    "        for row, nbr_row in zip(idx, nbrs):\n",
    "            for nbr_idx in nbr_row:\n",
    "                if nbr_idx == row:  # Skip self\n",
    "                    continue\n",
    "                nbr_pid = patch_ids[nbr_idx]\n",
    "                if nbr_pid != pid:  # Add different patch as neighbor\n",
    "                    patch_neighbours[pid].add(nbr_pid)\n",
    "\n",
    "    # For each patch and its neighbors, find the closest ghost points\n",
    "    patch_ghost_points = defaultdict(dict)\n",
    "    \n",
    "    # Determine if ghost points should be calculated dynamically\n",
    "    if n_ghost_points is None:\n",
    "        dynamic_n_ghost_points = True\n",
    "    else:\n",
    "        dynamic_n_ghost_points = False\n",
    "\n",
    "    for pid in unique_patches:\n",
    "        idx = patch_indices[pid]\n",
    "        coords_patch = coords_all[idx]\n",
    "\n",
    "        # Calculate number of ghost points based on patch size if not specified\n",
    "        if dynamic_n_ghost_points:\n",
    "            n_ghost_points = int(len(idx) * ghost_points_ratio)\n",
    "\n",
    "        for nbr_pid in patch_neighbours[pid]:\n",
    "            nbr_idx = patch_indices[nbr_pid]\n",
    "            coords_nbr = coords_all[nbr_idx]\n",
    "            \n",
    "            # Build KDTree for neighbor patch for efficient querying\n",
    "            nbr_tree = cKDTree(coords_nbr)\n",
    "            \n",
    "            # Find closest points in neighbor patch to current patch\n",
    "            k_val = min(n_ghost_points, len(nbr_idx))\n",
    "            dists, ghost_indices = nbr_tree.query(coords_patch, k=k_val)\n",
    "\n",
    "            # Flatten distance and index arrays for processing\n",
    "            dists_flat = dists.flatten() if dists.ndim > 1 else dists\n",
    "            ghost_indices_flat = ghost_indices.flatten() if ghost_indices.ndim > 1 else ghost_indices\n",
    "\n",
    "            # Sort ghost points by their indices within the neighbor patch\n",
    "            sorted_idx = np.argsort(ghost_indices_flat)\n",
    "            ghost_indices_sorted = ghost_indices_flat[sorted_idx]\n",
    "            dists_sorted = dists_flat[sorted_idx]\n",
    "\n",
    "            # Select unique ghost points with shortest distances\n",
    "            # This ensures we don't select the same ghost point multiple times\n",
    "            unique_ghost_indices, unique_pos = np.unique(ghost_indices_sorted, return_index=True)\n",
    "            \n",
    "            if len(unique_ghost_indices) > 0:\n",
    "                # Get distances for unique indices and sort by distance\n",
    "                unique_dists = dists_sorted[unique_pos]\n",
    "                dist_order = np.argsort(unique_dists)\n",
    "                selected_indices = unique_ghost_indices[dist_order][:n_ghost_points]\n",
    "            else:\n",
    "                selected_indices = np.array([], dtype=int)\n",
    "\n",
    "            # Convert local neighbor indices to global indices\n",
    "            ghost_global_indices = np.array(nbr_idx)[selected_indices]\n",
    "            patch_ghost_points[pid][nbr_pid] = ghost_global_indices\n",
    "\n",
    "    return patch_ids, slice_groups, patch_neighbours, patch_ghost_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Patching on a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with file: 0000.csv\n",
      "Data shape: (61360, 10)\n",
      "Columns: ['node', 'ts', 'time (d)', 'X', 'Y', 'Z', 'slice', 'mass_concentration', 'pressure', 'head']\n",
      "Data range - X: [355702.40, 358345.95]\n",
      "Data range - Y: [6456013.44, 6459170.98]\n",
      "Data range - Z: [-40.00, 33.87]\n",
      "Head range: [0.00, 0.98]\n",
      "Head range: [50.00, 35000.00]\n"
     ]
    }
   ],
   "source": [
    "# Test interpolation on a single file first\n",
    "test_file = ts_files[0]\n",
    "print(f\"Testing with file: {test_file}\")\n",
    "\n",
    "# Load data\n",
    "res_df = pd.read_csv(os.path.join(raw_data_dir, test_file))\n",
    "print(f\"Data shape: {res_df.shape}\")\n",
    "print(f\"Columns: {res_df.columns.tolist()}\")\n",
    "print(f\"Data range - X: [{res_df.X.min():.2f}, {res_df.X.max():.2f}]\")\n",
    "print(f\"Data range - Y: [{res_df.Y.min():.2f}, {res_df.Y.max():.2f}]\")\n",
    "print(f\"Data range - Z: [{res_df.Z.min():.2f}, {res_df.Z.max():.2f}]\")\n",
    "print(f\"Head range: [{res_df['head'].min():.2f}, {res_df['head'].max():.2f}]\")\n",
    "print(f\"Head range: [{res_df['mass_concentration'].min():.2f}, {res_df['mass_concentration'].max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize x, y, z to a 0, 1 range\n",
    "x_norm = (res_df['X'] - res_df['X'].min()) / (res_df['X'].max() - res_df['X'].min())\n",
    "y_norm = (res_df['Y'] - res_df['Y'].min()) / (res_df['Y'].max() - res_df['Y'].min())\n",
    "z_norm = (res_df['Z'] - res_df['Z'].min()) / (res_df['Z'].max() - res_df['Z'].min())\n",
    "\n",
    "# Optionally, add normalized columns to the dataframe for later use\n",
    "res_df['X_norm'] = x_norm\n",
    "res_df['Y_norm'] = y_norm\n",
    "res_df['Z_norm'] = z_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slice group 1: [1 2 3 4 5 6 7]\n",
      "Processing slice group 2: [ 8  9 10 11 12 13 14]\n",
      "Processing slice group 3: [15 16 17 18 19 20]\n",
      "Processing slice group 4: [21 22 23 24 25 26]\n",
      "\n",
      "Examining patch 1\n",
      "Number of neighbours: 6\n",
      "Number of ghost points: 630\n",
      "Number of points in patch: 2117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_7.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examining patch 2\n",
      "Number of neighbours: 6\n",
      "Number of ghost points: 954\n",
      "Number of points in patch: 3182\n",
      "\n",
      "Examining patch 3\n",
      "Number of neighbours: 4\n",
      "Number of ghost points: 640\n",
      "Number of points in patch: 3202\n",
      "\n",
      "Examining patch 4\n",
      "Number of neighbours: 6\n",
      "Number of ghost points: 750\n",
      "Number of points in patch: 2510\n",
      "\n",
      "Examining patch 5\n",
      "Number of neighbours: 3\n",
      "Number of ghost points: 825\n",
      "Number of points in patch: 5509\n",
      "\n",
      "Examining patch 6\n",
      "Number of neighbours: 11\n",
      "Number of ghost points: 1793\n",
      "Number of points in patch: 3263\n",
      "\n",
      "Examining patch 7\n",
      "Number of neighbours: 5\n",
      "Number of ghost points: 1125\n",
      "Number of points in patch: 4516\n",
      "\n",
      "Examining patch 8\n",
      "Number of neighbours: 6\n",
      "Number of ghost points: 1074\n",
      "Number of points in patch: 3594\n",
      "\n",
      "Examining patch 9\n",
      "Number of neighbours: 7\n",
      "Number of ghost points: 364\n",
      "Number of points in patch: 1042\n",
      "\n",
      "Examining patch 10\n",
      "Number of neighbours: 6\n",
      "Number of ghost points: 1230\n",
      "Number of points in patch: 4105\n",
      "\n",
      "Examining patch 11\n",
      "Number of neighbours: 7\n",
      "Number of ghost points: 959\n",
      "Number of points in patch: 2743\n",
      "\n",
      "Examining patch 12\n",
      "Number of neighbours: 7\n",
      "Number of ghost points: 700\n",
      "Number of points in patch: 2013\n",
      "\n",
      "Examining patch 13\n",
      "Number of neighbours: 7\n",
      "Number of ghost points: 1274\n",
      "Number of points in patch: 3647\n",
      "\n",
      "Examining patch 14\n",
      "Number of neighbours: 4\n",
      "Number of ghost points: 604\n",
      "Number of points in patch: 3030\n",
      "\n",
      "Examining patch 15\n",
      "Number of neighbours: 5\n",
      "Number of ghost points: 680\n",
      "Number of points in patch: 2727\n",
      "\n",
      "Examining patch 16\n",
      "Number of neighbours: 4\n",
      "Number of ghost points: 728\n",
      "Number of points in patch: 3654\n",
      "\n",
      "Examining patch 17\n",
      "Number of neighbours: 4\n",
      "Number of ghost points: 404\n",
      "Number of points in patch: 2022\n",
      "\n",
      "Examining patch 18\n",
      "Number of neighbours: 3\n",
      "Number of ghost points: 411\n",
      "Number of points in patch: 2742\n",
      "\n",
      "Examining patch 19\n",
      "Number of neighbours: 3\n",
      "Number of ghost points: 453\n",
      "Number of points in patch: 3024\n",
      "\n",
      "Examining patch 20\n",
      "Number of neighbours: 4\n",
      "Number of ghost points: 540\n",
      "Number of points in patch: 2718\n"
     ]
    }
   ],
   "source": [
    "# Choose number of patches\n",
    "n_patches = 20\n",
    "slice_split = 4\n",
    "ghost_points_ratio = 0.05\n",
    "\n",
    "# Create patches\n",
    "patch_ids, slice_groups, patch_neighbours, patch_ghost_points = create_patches(\n",
    "    x=res_df.X_norm.values,\n",
    "    y=res_df.Y_norm.values,\n",
    "    z=res_df.Z_norm.values,\n",
    "    slice_id=res_df.slice.values,\n",
    "    n_patches=n_patches,\n",
    "    slice_split=slice_split,\n",
    "    ghost_points_ratio=ghost_points_ratio\n",
    ")\n",
    "\n",
    "# Add patch ids to dataframe\n",
    "res_df['patch_id'] = patch_ids.astype(str)\n",
    "res_df['slice_group'] = slice_groups.astype(str)\n",
    "\n",
    "# Analyse ghost points\n",
    "for patch_id in np.unique(patch_ids):\n",
    "\n",
    "    print(f\"\\nExamining patch {patch_id}\")\n",
    "    print(f\"Number of neighbours: {len(patch_neighbours[patch_id])}\")\n",
    "\n",
    "    # Get the neighbours and ghost points for the patch\n",
    "    neighbours = patch_neighbours[patch_id]\n",
    "    ghost_points_by_patch = patch_ghost_points[patch_id]\n",
    "\n",
    "    # Plot the points in patch_id\n",
    "    points_to_plot = res_df[res_df['patch_id'] == str(patch_id)]\n",
    "\n",
    "    # Add the ghost points\n",
    "    ghost_points = []\n",
    "    for k, v in ghost_points_by_patch.items():\n",
    "        ghost_points.extend(v)\n",
    "    ghost_points = np.array(ghost_points)\n",
    "\n",
    "    print(f\"Number of ghost points: {len(ghost_points)}\")\n",
    "    print(f\"Number of points in patch: {len(points_to_plot)}\")\n",
    "\n",
    "    # Add the ghost points to the points to plot\n",
    "    points_to_plot = pd.concat([points_to_plot, res_df.loc[ghost_points]])\n",
    "\n",
    "    # Plot the points in patch_id\n",
    "    if patch_id == 1:\n",
    "\n",
    "        # Plot the points in patch_id along with the neighbours and ghost points\n",
    "        fig = px.scatter_3d(\n",
    "            points_to_plot,\n",
    "            x='X_norm',\n",
    "            y='Y_norm',\n",
    "            z='Z_norm',\n",
    "            color='patch_id',\n",
    "            opacity=0.8,\n",
    "            size_max=10,\n",
    "            title='Patches (clusters) in 3D'\n",
    "        )\n",
    "\n",
    "        # Update the traces\n",
    "        fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "        # Add the title and axis labels\n",
    "        fig.update_layout(\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            ),\n",
    "            legend_title_text='Patch ID'\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_data = {}\n",
    "\n",
    "for patch_id in np.unique(patch_ids):\n",
    "\n",
    "    # Get the neighbours and ghost points for the patch\n",
    "    neighbours = patch_neighbours[patch_id]\n",
    "    ghost_points_by_patch = patch_ghost_points[patch_id]\n",
    "\n",
    "    # Plot the points in patch_id\n",
    "    patch_points = res_df[res_df['patch_id'] == str(patch_id)]\n",
    "\n",
    "    # Add the ghost points\n",
    "    ghost_points = []\n",
    "    for k, v in ghost_points_by_patch.items():\n",
    "        ghost_points.extend(v)\n",
    "    ghost_points = res_df.loc[np.array(ghost_points)]\n",
    "\n",
    "    # Add the data to the patch_data list\n",
    "    patch_data[int(patch_id)] = {\n",
    "        'slice_group': int(slice_groups[patch_id]),\n",
    "        'neighbour_patches': np.array(list(map(int, neighbours))),\n",
    "        'ghost_nodes': ghost_points['node'].unique(),\n",
    "        'core_nodes': patch_points['node'].unique(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define json file path\n",
    "patch_data_json = os.path.join(base_data_dir, 'patches.json')\n",
    "\n",
    "# Write the patch data to a json file\n",
    "with open(patch_data_json, 'w') as f:\n",
    "    json.dump(patch_data, f, indent=2, default=lambda x: x.tolist() if hasattr(x, 'tolist') else list(x) if hasattr(x, '__iter__') and not isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation successful: 20 patches loaded and validated.\n"
     ]
    }
   ],
   "source": [
    "# Read the patch_data_json file and validate the data\n",
    "\n",
    "with open(patch_data_json, 'r') as f:\n",
    "    loaded_patch_data = json.load(f)\n",
    "\n",
    "# Validation: check structure and types\n",
    "def validate_patch_data(patch_data_dict):\n",
    "    required_keys = {'slice_group', 'neighbour_patches', 'ghost_nodes', 'core_nodes'}\n",
    "    if not isinstance(patch_data_dict, dict):\n",
    "        raise TypeError(\"Loaded patch data should be a dict (from JSON), got type: {}\".format(type(patch_data_dict)))\n",
    "    for patch_id, patch in patch_data_dict.items():\n",
    "        # Check all required keys are present\n",
    "        if not required_keys.issubset(patch.keys()):\n",
    "            raise ValueError(f\"Patch {patch_id} missing required keys: {required_keys - set(patch.keys())}\")\n",
    "        # Check types\n",
    "        if not isinstance(patch_id, str) and not isinstance(patch_id, int):\n",
    "            raise TypeError(f\"Patch id {patch_id} is not str or int\")\n",
    "        if not isinstance(patch['slice_group'], int):\n",
    "            raise TypeError(f\"Patch {patch_id} 'slice_group' is not int\")\n",
    "        if not (isinstance(patch['neighbour_patches'], list) and all(isinstance(x, int) for x in patch['neighbour_patches'])):\n",
    "            raise TypeError(f\"Patch {patch_id} 'neighbour_patches' is not list of int\")\n",
    "        if not (isinstance(patch['ghost_nodes'], list) and all(isinstance(x, (int, str)) for x in patch['ghost_nodes'])):\n",
    "            raise TypeError(f\"Patch {patch_id} 'ghost_nodes' is not list of int or str\")\n",
    "        if not (isinstance(patch['core_nodes'], list) and all(isinstance(x, (int, str)) for x in patch['core_nodes'])):\n",
    "            raise TypeError(f\"Patch {patch_id} 'core_nodes' is not list of int or str\")\n",
    "    print(f\"Validation successful: {len(patch_data_dict)} patches loaded and validated.\")\n",
    "\n",
    "validate_patch_data(loaded_patch_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_21.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add patch ids to dataframe for convenience\n",
    "res_df['patch_id'] = patch_ids.astype(str)\n",
    "res_df['slice_group'] = slice_groups\n",
    "\n",
    "# Use a discrete color space for patch coloring\n",
    "# We'll use Plotly's qualitative color sets, repeating as needed\n",
    "discrete_colors = px.colors.qualitative.Dark24_r  # or Set1, Set2, etc.\n",
    "if n_patches > len(discrete_colors):\n",
    "    # Repeat the palette to cover all patches\n",
    "    color_discrete_sequence = (discrete_colors * ((n_patches // len(discrete_colors)) + 1))[:n_patches]\n",
    "else:\n",
    "    color_discrete_sequence = discrete_colors[:n_patches]\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    res_df,\n",
    "    x='X_norm',\n",
    "    y='Y_norm',\n",
    "    z='Z_norm',\n",
    "    color='patch_id',\n",
    "    color_discrete_sequence=color_discrete_sequence,\n",
    "    opacity=0.7,\n",
    "    size_max=10,\n",
    "    title='Patches (clusters) in 3D'\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ),\n",
    "    legend_title_text='Patch ID'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (torch-env)",
   "language": "python",
   "name": "conda_torch-env_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
