{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb318d83-7355-44c0-8b1a-871d5b984c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('/srv/scratch/z5370003/projects/src/04_groundwater/variable_density')\n",
    "\n",
    "from src.data.transform import Normalize\n",
    "from src.data.patch_dataset_multi_col import GWPatchDatasetMultiCol\n",
    "from src.data.batch_sampler import PatchBatchSampler\n",
    "from src.models.neuralop.gino import GINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef3bf6e",
   "metadata": {},
   "source": [
    "# GINO Model Predictions Analysis\n",
    "\n",
    "This notebook loads a trained GINO multi-column model and datasets for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3736cc6",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f48826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Target columns: ['mass_concentration', 'head']\n",
      "Target column indices: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "base_data_dir = '/srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/'\n",
    "raw_data_dir = os.path.join(base_data_dir, 'all')\n",
    "patch_data_dir = os.path.join(base_data_dir, 'filter_patch')\n",
    "model_path = '/srv/scratch/z5370003/projects/results/04_groundwater/variable_density/GINO/multi_col/mass_conc_head/exp_lr4.8e4_exp_bs512/gino_multi_20251010_092951/checkpoints/latest_checkpoint.pth'  # UPDATE THIS\n",
    "\n",
    "# Model configuration\n",
    "target_cols = ['mass_concentration', 'head']  # UPDATE THIS\n",
    "input_window_size = 5\n",
    "output_window_size = 5\n",
    "batch_size = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Target column indices\n",
    "names_to_idx = {\n",
    "    'mass_concentration': 0,\n",
    "    'head': 1,\n",
    "    'pressure': 2\n",
    "}\n",
    "target_col_indices = [names_to_idx[col] for col in target_cols]\n",
    "n_target_cols = len(target_cols)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Target columns: {target_cols}\")\n",
    "print(f\"Target column indices: {target_col_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984358b6",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08f9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate mean: [ 3.57225665e+05  6.45774324e+06 -9.27782248e+00]\n",
      "Coordinate std: [569.1699999  566.35797379  15.26565618]\n",
      "Observation mean: [1.77942252e+04 3.95881156e-01 9.48469883e+01]\n",
      "Observation std: [1.55859465e+04 2.13080032e-01 1.51226320e+02]\n"
     ]
    }
   ],
   "source": [
    "# Calculate coordinate normalization\n",
    "df = pd.read_csv(os.path.join(raw_data_dir, '0000.csv'))\n",
    "coord_mean = df[['X', 'Y', 'Z']].mean().values\n",
    "coord_std = df[['X', 'Y', 'Z']].std().values\n",
    "coord_transform = Normalize(mean=coord_mean, std=coord_std)\n",
    "print(f\"Coordinate mean: {coord_mean}\")\n",
    "print(f\"Coordinate std: {coord_std}\")\n",
    "\n",
    "# Calculate observation normalization\n",
    "target_obs_cols = ['mass_concentration', 'head', 'pressure']\n",
    "obs_mean = df[target_obs_cols].mean().values\n",
    "obs_std = df[target_obs_cols].std().values\n",
    "obs_transform = Normalize(mean=obs_mean, std=obs_std)\n",
    "print(f\"Observation mean: {obs_mean}\")\n",
    "print(f\"Observation std: {obs_std}\")\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbb5b9",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2a820a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 13180\n",
      "Val dataset size: 5560\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset\n",
    "train_ds = GWPatchDatasetMultiCol(\n",
    "    data_path=patch_data_dir,\n",
    "    dataset='train',\n",
    "    coord_transform=coord_transform,\n",
    "    obs_transform=obs_transform,\n",
    "    input_window_size=input_window_size,\n",
    "    output_window_size=output_window_size,\n",
    "    target_col_indices=target_col_indices,\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "val_ds = GWPatchDatasetMultiCol(\n",
    "    data_path=patch_data_dir,\n",
    "    dataset='val',\n",
    "    coord_transform=coord_transform,\n",
    "    obs_transform=obs_transform,\n",
    "    input_window_size=input_window_size,\n",
    "    output_window_size=output_window_size,\n",
    "    target_col_indices=target_col_indices,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print(f\"Val dataset size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bdac6-b822-408c-820f-5b16440286ed",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06087a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pbs.7134913.kman.restech.unsw.edu.au/ipykernel_3942624/2399224792.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model configuration:\n",
      "- FNO modes: (12, 12, 8)\n",
      "- FNO layers: 4\n",
      "- Hidden channels: 64\n",
      "- GNO radius: 0.15\n",
      "- Latent dims: (32, 32, 24)\n",
      "- Target columns: ['mass_concentration', 'head']\n",
      "- Number of target columns: 2\n",
      "\n",
      "Checkpoint training progress:\n",
      "- Epoch: 250\n",
      "- Train loss: 0.2006\n",
      "- Val loss: 0.2706\n",
      "\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "saved_args = checkpoint['args']\n",
    "\n",
    "print(\"Saved model configuration:\")\n",
    "print(f\"- FNO modes: {saved_args.fno_n_modes}\")\n",
    "print(f\"- FNO layers: {saved_args.fno_n_layers}\")\n",
    "print(f\"- Hidden channels: {saved_args.fno_hidden_channels}\")\n",
    "print(f\"- GNO radius: {saved_args.gno_radius}\")\n",
    "print(f\"- Latent dims: {saved_args.latent_query_dims}\")\n",
    "print(f\"- Target columns: {saved_args.target_cols}\")\n",
    "print(f\"- Number of target columns: {saved_args.n_target_cols}\")\n",
    "\n",
    "# Initialize model\n",
    "model = GINO(\n",
    "    in_gno_coord_dim=saved_args.coord_dim,\n",
    "    in_gno_radius=saved_args.gno_radius,\n",
    "    in_gno_out_channels=saved_args.in_gno_out_channels,\n",
    "    in_gno_channel_mlp_layers=saved_args.in_gno_channel_mlp_layers,\n",
    "    fno_n_layers=saved_args.fno_n_layers,\n",
    "    fno_n_modes=saved_args.fno_n_modes,\n",
    "    fno_hidden_channels=saved_args.fno_hidden_channels,\n",
    "    lifting_channels=saved_args.lifting_channels,\n",
    "    out_gno_coord_dim=saved_args.coord_dim,\n",
    "    out_gno_radius=saved_args.gno_radius,\n",
    "    out_gno_channel_mlp_layers=saved_args.out_gno_channel_mlp_layers,\n",
    "    projection_channel_ratio=saved_args.projection_channel_ratio,\n",
    "    out_channels=saved_args.out_channels,\n",
    ").to(device)\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nCheckpoint training progress:\")\n",
    "print(f\"- Epoch: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"- Train loss: {checkpoint['train_losses'][-1]:.4f}\")\n",
    "print(f\"- Val loss: {checkpoint['val_losses'][-1]:.4f}\")\n",
    "print(f\"\\nModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ced130",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9341552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def make_collate_fn(latent_query_dims=(32, 32, 24), coord_dim=3, device='cuda'):\n",
    "    \"\"\"Create collate function for batch processing (same as in generate_gino_predictions_multi_col.py).\"\"\"\n",
    "    def collate_fn(batch_samples):\n",
    "        # Get shared point cloud from first sample\n",
    "        core_coords = batch_samples[0]['core_coords']\n",
    "        ghost_coords = batch_samples[0]['ghost_coords']\n",
    "        \n",
    "        # Combine core and ghost points\n",
    "        point_coords = torch.concat([core_coords, ghost_coords], dim=0).float()\n",
    "        \n",
    "        # Create latent queries grid over the per-batch bounding box\n",
    "        coords_min = torch.min(point_coords, dim=0).values\n",
    "        coords_max = torch.max(point_coords, dim=0).values\n",
    "        latent_query_arr = [\n",
    "            torch.linspace(coords_min[i], coords_max[i], latent_query_dims[i], device=device)\n",
    "            for i in range(coord_dim)\n",
    "        ]\n",
    "        latent_queries = torch.stack(torch.meshgrid(*latent_query_arr, indexing='ij'), dim=-1)\n",
    "        \n",
    "        # Build batched sequences\n",
    "        x_list, y_list = [], []\n",
    "        for sample in batch_samples:\n",
    "            sample_input = torch.concat([sample['core_in'], sample['ghost_in']], dim=0).float().unsqueeze(0)\n",
    "            sample_output = torch.concat([sample['core_out'], sample['ghost_out']], dim=0).float().unsqueeze(0)\n",
    "            x_list.append(sample_input)\n",
    "            y_list.append(sample_output)\n",
    "        \n",
    "        x = torch.cat(x_list, dim=0)  # [B, N_points, input_window_size * n_target_cols]\n",
    "        y = torch.cat(y_list, dim=0)  # [B, N_points, output_window_size * n_target_cols]\n",
    "        \n",
    "        return {\n",
    "            'point_coords': point_coords,\n",
    "            'latent_queries': latent_queries,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'core_len': len(core_coords),\n",
    "            'patch_id': sample['patch_id']\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def reshape_multi_col_predictions(predictions, output_window_size, n_target_cols):\n",
    "    \"\"\"\n",
    "    Reshape concatenated predictions to separate target columns.\n",
    "    \n",
    "    The dataset concatenates data as: [t0_var0, t0_var1, t1_var0, t1_var1, t2_var0, t2_var1, ...]\n",
    "    This is because _concat_sequence does: seq.reshape(n_points, -1) on [n_points, window_size, n_target_cols]\n",
    "    which flattens in row-major order, interleaving timesteps and variables.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Array of shape [N_samples, N_points, output_window_size * n_target_cols]\n",
    "        output_window_size: Number of timesteps\n",
    "        n_target_cols: Number of target columns\n",
    "        \n",
    "    Returns:\n",
    "        Array of shape [N_samples, N_points, output_window_size, n_target_cols]\n",
    "    \"\"\"\n",
    "    n_samples, n_points, total_size = predictions.shape\n",
    "    # From [N_samples, N_points, T*C] to [N_samples, N_points, T, C]\n",
    "    # where T = output_window_size and C = n_target_cols\n",
    "    if total_size != output_window_size * n_target_cols:\n",
    "        raise ValueError(f\"Expected total size {output_window_size * n_target_cols} (output_window_size={output_window_size} * n_target_cols={n_target_cols}), but got {total_size}\")\n",
    "    \n",
    "    # The data is stored as [t0_v0, t0_v1, t1_v0, t1_v1, ...] for each point\n",
    "    # So we reshape to [N_samples, N_points, output_window_size, n_target_cols] directly\n",
    "    # This naturally deinterleaves the timesteps and variables\n",
    "    reshaped = predictions.reshape(n_samples, n_points, output_window_size, n_target_cols)\n",
    "    return reshaped\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1f8e5",
   "metadata": {},
   "source": [
    "## Example: Get a single batch for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979e73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building patch groups (one-time operation)...\n",
      "Building patch_ids cache...\n",
      "Cached 13180 patch_ids\n",
      "Found 20 patches with 13180 total samples\n",
      "Patch sizes: min=659, max=659, avg=659.0\n",
      "Pre-built 420 batches\n",
      "Point coords shape: torch.Size([540, 3])\n",
      "Latent queries shape: torch.Size([32, 32, 24, 3])\n",
      "Input x shape: torch.Size([32, 540, 10])\n",
      "Target y shape: torch.Size([32, 540, 10])\n",
      "Core length: 453\n",
      "Patch ID: 20\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Create a sampler and loader\n",
    "sampler = PatchBatchSampler(\n",
    "    train_ds, \n",
    "    batch_size=batch_size,\n",
    "    shuffle_within_batches=False,\n",
    "    shuffle_patches=True,\n",
    "    seed=int(time.time())\n",
    ")\n",
    "\n",
    "# Get latent query dimensions from the saved model config\n",
    "latent_query_dims = saved_args.latent_query_dims\n",
    "collate_fn = make_collate_fn(latent_query_dims=latent_query_dims, coord_dim=3, device=device)\n",
    "val_loader = DataLoader(train_ds, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "\n",
    "# Get a single batch\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "print(f\"Point coords shape: {batch['point_coords'].shape}\")\n",
    "print(f\"Latent queries shape: {batch['latent_queries'].shape}\")\n",
    "print(f\"Input x shape: {batch['x'].shape}\")\n",
    "print(f\"Target y shape: {batch['y'].shape}\")\n",
    "print(f\"Core length: {batch['core_len']}\")\n",
    "print(f\"Patch ID: {batch['patch_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fabea",
   "metadata": {},
   "source": [
    "## Example: Generate predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a0750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (32, 453, 10)\n",
      "Target shape: (32, 453, 10)\n",
      "Coords shape: (453, 3)\n",
      "Inputs shape: (32, 453, 10)\n",
      "\n",
      "Predictions reshaped: (32, 453, 5, 2)\n",
      "Target reshaped: (32, 453, 5, 2)\n",
      "Shape format: [batch_size, n_points, output_window_size, n_target_cols]\n"
     ]
    }
   ],
   "source": [
    "# Move batch to device\n",
    "point_coords = batch['point_coords'].to(device).float()\n",
    "latent_queries = batch['latent_queries'].to(device).float()\n",
    "x = batch['x'].to(device).float()\n",
    "y = batch['y'].to(device).float()\n",
    "core_len = batch['core_len']\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_geom=point_coords,\n",
    "        latent_queries=latent_queries,\n",
    "        x=x,\n",
    "        output_queries=point_coords,\n",
    "    )\n",
    "\n",
    "# Extract core points only (exclude ghost points)\n",
    "pred_obs = outputs[:, :core_len].cpu().numpy()\n",
    "target_obs = y[:, :core_len].cpu().numpy()\n",
    "coords = point_coords[:core_len].cpu().numpy()\n",
    "input_obs = x[:, :core_len].cpu().numpy()\n",
    "\n",
    "print(f\"Predictions shape: {pred_obs.shape}\")\n",
    "print(f\"Target shape: {target_obs.shape}\")\n",
    "print(f\"Coords shape: {coords.shape}\")\n",
    "print(f\"Inputs shape: {input_obs.shape}\")\n",
    "\n",
    "\n",
    "# Reshape to separate target columns\n",
    "pred_reshaped = reshape_multi_col_predictions(pred_obs, output_window_size, n_target_cols)\n",
    "target_reshaped = reshape_multi_col_predictions(target_obs, output_window_size, n_target_cols)\n",
    "input_reshaped = reshape_multi_col_predictions(input_obs, input_window_size, n_target_cols)\n",
    "\n",
    "print(f\"\\nPredictions reshaped: {pred_reshaped.shape}\")\n",
    "print(f\"Target reshaped: {target_reshaped.shape}\")\n",
    "print(f\"Shape format: [batch_size, n_points, output_window_size, n_target_cols]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55586f3f",
   "metadata": {},
   "source": [
    "## Your Analysis Code\n",
    "\n",
    "Add your analysis cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f93f18-df7f-4ae8-8382-772b05d95aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb286eb6-4f28-4681-843b-f0f1670c9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1 ts\n",
      "    Error with target: 0.0248\n",
      "    Error with input: 0.0240\n",
      "At 2 ts\n",
      "    Error with target: 0.0604\n",
      "    Error with input: 0.0252\n",
      "At 3 ts\n",
      "    Error with target: 0.0790\n",
      "    Error with input: 0.0338\n",
      "At 4 ts\n",
      "    Error with target: 0.0773\n",
      "    Error with input: 0.0404\n",
      "At 5 ts\n",
      "    Error with target: 0.0789\n",
      "    Error with input: 0.0426\n"
     ]
    }
   ],
   "source": [
    "for i in range(output_window_size):\n",
    "    print(f\"At {i+1} ts\")\n",
    "    for j in range(1, n_target_cols):\n",
    "        # print(f\"  for variable {target_cols[j]}\")\n",
    "        pred_at_ts = pred_reshaped[:, :, i, j]\n",
    "        target_at_ts = target_reshaped[:, :, i, j]\n",
    "        input_at_last_ts = input_reshaped[:, :, -1, j]\n",
    "\n",
    "        err_w_target = mean_squared_error(pred_at_ts.reshape(-1, 1), target_at_ts.reshape(-1, 1))\n",
    "        err_w_input = mean_squared_error(pred_at_ts.reshape(-1, 1), input_at_last_ts.reshape(-1, 1))\n",
    "\n",
    "\n",
    "        print(f\"    Error with target: {err_w_target:.4f}\")\n",
    "        print(f\"    Error with input: {err_w_input:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835e99e-19a2-4f7a-b58a-3b4e19df6da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9baf4a0a-12f1-48d5-801f-33affc9f490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1 ts\n",
      "    Error with target: 0.0006\n",
      "    Error with input: 0.0006\n",
      "At 2 ts\n",
      "    Error with target: 0.0006\n",
      "    Error with input: 0.0006\n",
      "At 3 ts\n",
      "    Error with target: 0.0006\n",
      "    Error with input: 0.0006\n",
      "At 4 ts\n",
      "    Error with target: 0.0006\n",
      "    Error with input: 0.0006\n",
      "At 5 ts\n",
      "    Error with target: 0.0006\n",
      "    Error with input: 0.0006\n"
     ]
    }
   ],
   "source": [
    "for i in range(output_window_size):\n",
    "    print(f\"At {i+1} ts\")\n",
    "    for j in range(0, n_target_cols-1):\n",
    "        # print(f\"  for variable {target_cols[j]}\")\n",
    "        pred_at_ts = pred_reshaped[:, :, i, j]\n",
    "        target_at_ts = target_reshaped[:, :, i, j]\n",
    "        input_at_last_ts = input_reshaped[:, :, -1, j]\n",
    "\n",
    "        err_w_target = mean_squared_error(pred_at_ts.reshape(-1, 1), target_at_ts.reshape(-1, 1))\n",
    "        err_w_input = mean_squared_error(pred_at_ts.reshape(-1, 1), input_at_last_ts.reshape(-1, 1))\n",
    "\n",
    "\n",
    "        print(f\"    Error with target: {err_w_target:.4f}\")\n",
    "        print(f\"    Error with input: {err_w_input:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8605b-e69e-4488-8e47-a512dc6ab5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (torch-env)",
   "language": "python",
   "name": "conda_torch-env_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
