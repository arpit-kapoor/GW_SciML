{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5403e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998229b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base data directory: /srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/\n",
      "Raw data directory: /srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/all\n",
      "Filtered data directory: /srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/filter_all_ts\n",
      "Forcings data directory: /srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/forcings_corrected\n"
     ]
    }
   ],
   "source": [
    "# Define data directories\n",
    "base_data_dir = '/srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/'\n",
    "# base_data_dir = '/Users/arpitkapoor/Library/CloudStorage/OneDrive-UNSW/Shared/Projects/01_PhD/05_groundwater/data/FEFLOW/variable_density'  # Uncomment for local testing\n",
    "raw_data_dir = os.path.join(base_data_dir, 'all')\n",
    "filtered_data_dir = os.path.join(base_data_dir, 'filter_all_ts')\n",
    "forcings_data_dir = os.path.join(base_data_dir, 'forcings_corrected')\n",
    "\n",
    "print(f\"Base data directory: {base_data_dir}\")\n",
    "print(f\"Raw data directory: {raw_data_dir}\")\n",
    "print(f\"Filtered data directory: {filtered_data_dir}\")\n",
    "print(f\"Forcings data directory: {forcings_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679c83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 1909\n",
      "First 3 files: ['0000.csv', '0001.csv', '0002.csv']\n",
      "Last 3 files: ['1906.csv', '1907.csv', '1908.csv']\n"
     ]
    }
   ],
   "source": [
    "# Get and sort time series files\n",
    "ts_files = sorted(os.listdir(filtered_data_dir))\n",
    "print(f\"Total number of files: {len(ts_files)}\")\n",
    "print(f\"First 3 files: {ts_files[:3]}\")\n",
    "print(f\"Last 3 files: {ts_files[-3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37dbc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define json file path\n",
    "patch_config_json = os.path.join(base_data_dir, 'patches.json')\n",
    "\n",
    "with open(patch_config_json, 'r') as f:\n",
    "    patch_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ef86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_patch_data_dir = os.path.join(base_data_dir, 'filter_patch_all_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9f93dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing patch 1\n",
      "Patch 1 has 2117 core nodes and 630 ghost nodes\n",
      "Patch 1 has 6 neighbour patches\n",
      "Patch 1 has 1 slice group\n",
      "\n",
      "Processing patch 2\n",
      "Patch 2 has 3182 core nodes and 954 ghost nodes\n",
      "Patch 2 has 6 neighbour patches\n",
      "Patch 2 has 1 slice group\n",
      "\n",
      "Processing patch 3\n",
      "Patch 3 has 3202 core nodes and 640 ghost nodes\n",
      "Patch 3 has 4 neighbour patches\n",
      "Patch 3 has 1 slice group\n",
      "\n",
      "Processing patch 4\n",
      "Patch 4 has 2510 core nodes and 750 ghost nodes\n",
      "Patch 4 has 6 neighbour patches\n",
      "Patch 4 has 1 slice group\n",
      "\n",
      "Processing patch 5\n",
      "Patch 5 has 5509 core nodes and 825 ghost nodes\n",
      "Patch 5 has 3 neighbour patches\n",
      "Patch 5 has 1 slice group\n",
      "\n",
      "Processing patch 6\n",
      "Patch 6 has 3263 core nodes and 1793 ghost nodes\n",
      "Patch 6 has 11 neighbour patches\n",
      "Patch 6 has 1 slice group\n",
      "\n",
      "Processing patch 7\n",
      "Patch 7 has 4516 core nodes and 1125 ghost nodes\n",
      "Patch 7 has 5 neighbour patches\n",
      "Patch 7 has 1 slice group\n",
      "\n",
      "Processing patch 8\n",
      "Patch 8 has 3594 core nodes and 1074 ghost nodes\n",
      "Patch 8 has 6 neighbour patches\n",
      "Patch 8 has 1 slice group\n",
      "\n",
      "Processing patch 9\n",
      "Patch 9 has 1042 core nodes and 364 ghost nodes\n",
      "Patch 9 has 7 neighbour patches\n",
      "Patch 9 has 1 slice group\n",
      "\n",
      "Processing patch 10\n",
      "Patch 10 has 4105 core nodes and 1230 ghost nodes\n",
      "Patch 10 has 6 neighbour patches\n",
      "Patch 10 has 1 slice group\n",
      "\n",
      "Processing patch 11\n",
      "Patch 11 has 2743 core nodes and 959 ghost nodes\n",
      "Patch 11 has 7 neighbour patches\n",
      "Patch 11 has 1 slice group\n",
      "\n",
      "Processing patch 12\n",
      "Patch 12 has 2013 core nodes and 700 ghost nodes\n",
      "Patch 12 has 7 neighbour patches\n",
      "Patch 12 has 1 slice group\n",
      "\n",
      "Processing patch 13\n",
      "Patch 13 has 3647 core nodes and 1274 ghost nodes\n",
      "Patch 13 has 7 neighbour patches\n",
      "Patch 13 has 1 slice group\n",
      "\n",
      "Processing patch 14\n",
      "Patch 14 has 3030 core nodes and 604 ghost nodes\n",
      "Patch 14 has 4 neighbour patches\n",
      "Patch 14 has 1 slice group\n",
      "\n",
      "Processing patch 15\n",
      "Patch 15 has 2727 core nodes and 680 ghost nodes\n",
      "Patch 15 has 5 neighbour patches\n",
      "Patch 15 has 1 slice group\n",
      "\n",
      "Processing patch 16\n",
      "Patch 16 has 3654 core nodes and 728 ghost nodes\n",
      "Patch 16 has 4 neighbour patches\n",
      "Patch 16 has 1 slice group\n",
      "\n",
      "Processing patch 17\n",
      "Patch 17 has 2022 core nodes and 404 ghost nodes\n",
      "Patch 17 has 4 neighbour patches\n",
      "Patch 17 has 1 slice group\n",
      "\n",
      "Processing patch 18\n",
      "Patch 18 has 2742 core nodes and 411 ghost nodes\n",
      "Patch 18 has 3 neighbour patches\n",
      "Patch 18 has 1 slice group\n",
      "\n",
      "Processing patch 19\n",
      "Patch 19 has 3024 core nodes and 453 ghost nodes\n",
      "Patch 19 has 3 neighbour patches\n",
      "Patch 19 has 1 slice group\n",
      "\n",
      "Processing patch 20\n",
      "Patch 20 has 2718 core nodes and 540 ghost nodes\n",
      "Patch 20 has 4 neighbour patches\n",
      "Patch 20 has 1 slice group\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['mass_concentration', 'head', 'pressure']\n",
    "forcing_cols = ['mass_concentration_bc', 'head_bc', 'recharge_forcing', 'sea_level_forcing']\n",
    "coords_cols = ['X', 'Y', 'Z']\n",
    "\n",
    "patch_data = {}\n",
    "\n",
    "fillval = -999\n",
    "\n",
    "for k, v in patch_config.items():\n",
    "    \n",
    "    # Get the patch configuration\n",
    "    config = patch_config[k]\n",
    "\n",
    "    # Print patch information\n",
    "    print(f\"\\nProcessing patch {k}\")\n",
    "    print(f\"Patch {k} has {len(config['core_nodes'])} core nodes and {len(config['ghost_nodes'])} ghost nodes\")\n",
    "    print(f\"Patch {k} has {len(config['neighbour_patches'])} neighbour patches\")\n",
    "    print(f\"Patch {k} has {config['slice_group']} slice group\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    core_patch_data = []\n",
    "    ghost_patch_data = []\n",
    "\n",
    "    core_forcings_data = []\n",
    "    ghost_forcings_data = []\n",
    "\n",
    "    # Load the data#\n",
    "    for ts_file in ts_files:\n",
    "        ts_df = pd.read_csv(os.path.join(filtered_data_dir, ts_file))\n",
    "        core_patch_data.append(ts_df.loc[ts_df['node'].isin(config['core_nodes']), target_cols].values)\n",
    "        ghost_patch_data.append(ts_df.loc[ts_df['node'].isin(config['ghost_nodes']), target_cols].values)\n",
    "        \n",
    "        ts_forcings_df = pd.read_csv(os.path.join(forcings_data_dir, ts_file))\n",
    "        core_forcings_data.append(ts_forcings_df.loc[ts_forcings_df['node'].isin(config['core_nodes']), forcing_cols].values)\n",
    "        ghost_forcings_data.append(ts_forcings_df.loc[ts_forcings_df['node'].isin(config['ghost_nodes']), forcing_cols].values)\n",
    "\n",
    "        \n",
    "    # Convert to numpy arrays\n",
    "    core_patch_data = np.nan_to_num(np.array(core_patch_data))\n",
    "    ghost_patch_data = np.nan_to_num(np.array(ghost_patch_data))\n",
    "    core_forcings_data = np.nan_to_num(np.array(core_forcings_data))\n",
    "    ghost_forcings_data = np.nan_to_num(np.array(ghost_forcings_data))\n",
    "    \n",
    "    core_coords = ts_df.loc[ts_df['node'].isin(config['core_nodes']), coords_cols].values\n",
    "    ghost_coords = ts_df.loc[ts_df['node'].isin(config['ghost_nodes']), coords_cols].values\n",
    "\n",
    "    # Create directory for patch data\n",
    "    patch_dir_path = os.path.join(filter_patch_data_dir, f'patch_{int(k):03d}')\n",
    "    os.makedirs(patch_dir_path, exist_ok=True)\n",
    "\n",
    "    # Save the data\n",
    "    np.save(os.path.join(patch_dir_path, 'core_obs.npy'), core_patch_data)\n",
    "    np.save(os.path.join(patch_dir_path, 'ghost_obs.npy'), ghost_patch_data)\n",
    "    np.save(os.path.join(patch_dir_path, 'core_coords.npy'), core_coords)\n",
    "    np.save(os.path.join(patch_dir_path, 'ghost_coords.npy'), ghost_coords)\n",
    "    np.save(os.path.join(patch_dir_path, 'core_forcings.npy'), core_forcings_data)\n",
    "    np.save(os.path.join(patch_dir_path, 'ghost_forcings.npy'), ghost_forcings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef69c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_arr = []\n",
    "\n",
    "\n",
    "for k, v in patch_config.items():\n",
    "\n",
    "    patch_dir_path = os.path.join(filter_patch_data_dir, f'patch_{int(k):03d}')\n",
    "\n",
    "    core_forcings_data = np.load(os.path.join(patch_dir_path, 'core_forcings.npy'))\n",
    "    ghost_forcings_data = np.load(os.path.join(patch_dir_path, 'ghost_forcings.npy'))\n",
    "\n",
    "    # print((~np.isnan(core_forcings_data)).sum()/core_forcings_data.flatten().shape[0])\n",
    "    concat_arr.append(core_forcings_data.reshape(-1, 4))\n",
    "\n",
    "concat_arr = np.concatenate(concat_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc06dd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.48153188e+03,  9.42562257e-03,  3.84628900e-05, -8.05859849e-04]),\n",
       " array([7.04689144e+03, 8.24747365e-02, 7.88193443e-04, 5.29570033e-02]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(concat_arr, 0), np.nanstd(concat_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8188810-9543-4ec2-8761-41a6f8e462a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        , -0.65004828,  0.        , -0.651     ]),\n",
       " array([3.5000000e+04, 1.7949175e+00, 4.7559177e-02, 9.1500000e-01]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmin(concat_arr, 0), np.nanmax(concat_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f51c6-3db8-47c8-80d8-a40cd3cb5e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (torch-env)",
   "language": "python",
   "name": "conda_torch-env_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
