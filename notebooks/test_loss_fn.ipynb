{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6dcfea-e5d5-4afe-9013-e335bf90e6e1",
   "metadata": {},
   "source": [
    "# Test Custom Loss Function for GINO Model\n",
    "\n",
    "This notebook tests a custom loss function for the GINO multi-column model that can apply different loss weights or functions to individual target variables (e.g., mass_concentration vs head)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0868a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d1a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data.transform import Normalize\n",
    "from src.data.patch_dataset_multi_col import GWPatchDatasetMultiCol\n",
    "from src.data.batch_sampler import PatchBatchSampler\n",
    "from src.models.neuralop.gino import GINO\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7a66a",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4384a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "base_data_dir = '/srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density'\n",
    "raw_data_dir = os.path.join(base_data_dir, 'all')\n",
    "patch_data_dir = os.path.join(base_data_dir, 'filter_patch')\n",
    "\n",
    "# Model checkpoint path (update this to your trained model)\n",
    "model_path = '/srv/scratch/z5370003/projects/results/04_groundwater/variable_density/GINO/multi_col/mass_conc_head/exp_lr4.8e4_exp_bs512/gino_multi_20251010_092951/checkpoints/checkpoint_epoch_0009.pth'\n",
    "\n",
    "# Model parameters\n",
    "target_cols = ['mass_concentration', 'head']\n",
    "target_col_indices = [0, 1]  # Indices in the observation columns\n",
    "input_window_size = 5\n",
    "output_window_size = 5\n",
    "batch_size = 4  # Small batch for testing\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441020c",
   "metadata": {},
   "source": [
    "## 3. Load Data Transforms and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401423e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate mean: [ 3.57225665e+05  6.45774324e+06 -9.27782248e+00]\n",
      "Coordinate std: [569.1699999  566.35797379  15.26565618]\n",
      "Observation mean: [1.77942252e+04 3.95881156e-01 9.48469883e+01]\n",
      "Observation std: [1.55859465e+04 2.13080032e-01 1.51226320e+02]\n"
     ]
    }
   ],
   "source": [
    "# Calculate transforms\n",
    "df = pd.read_csv(os.path.join(raw_data_dir, '0000.csv'))\n",
    "\n",
    "# Coordinate transform\n",
    "coord_mean = df[['X', 'Y', 'Z']].mean().values\n",
    "coord_std = df[['X', 'Y', 'Z']].std().values\n",
    "coord_transform = Normalize(mean=coord_mean, std=coord_std)\n",
    "\n",
    "# Observation transform\n",
    "target_obs_cols = ['mass_concentration', 'head', 'pressure']\n",
    "obs_mean = df[target_obs_cols].mean().values\n",
    "obs_std = df[target_obs_cols].std().values\n",
    "obs_transform = Normalize(mean=obs_mean, std=obs_std)\n",
    "\n",
    "print(f\"Coordinate mean: {coord_mean}\")\n",
    "print(f\"Coordinate std: {coord_std}\")\n",
    "print(f\"Observation mean: {obs_mean}\")\n",
    "print(f\"Observation std: {obs_std}\")\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb618a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 13180\n",
      "Target columns: ['mass_concentration', 'head']\n",
      "Target column indices: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset\n",
    "train_ds = GWPatchDatasetMultiCol(\n",
    "    data_path=patch_data_dir,\n",
    "    dataset='train',\n",
    "    coord_transform=coord_transform,\n",
    "    obs_transform=obs_transform,\n",
    "    input_window_size=input_window_size,\n",
    "    output_window_size=output_window_size,\n",
    "    target_col_indices=target_col_indices,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset length: {len(train_ds)}\")\n",
    "print(f\"Target columns: {target_cols}\")\n",
    "print(f\"Target column indices: {target_col_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf1f6c",
   "metadata": {},
   "source": [
    "## 4. Create DataLoader with Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7e98133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building patch groups (one-time operation)...\n",
      "Building patch_ids cache...\n",
      "Cached 13180 patch_ids\n",
      "Found 20 patches with 13180 total samples\n",
      "Patch sizes: min=659, max=659, avg=659.0\n",
      "Pre-built 3300 batches\n",
      "Number of batches: 3300\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "coord_dim = 3\n",
    "latent_query_dims = (32, 32, 24)\n",
    "n_target_cols = len(target_cols)\n",
    "\n",
    "def make_collate_fn():\n",
    "    \"\"\"Create collate function for batch processing.\"\"\"\n",
    "    def collate_fn(batch_samples):\n",
    "        # Get shared point cloud from first sample\n",
    "        core_coords = batch_samples[0]['core_coords']\n",
    "        ghost_coords = batch_samples[0]['ghost_coords']\n",
    "        \n",
    "        # Combine core and ghost points\n",
    "        point_coords = torch.concat([core_coords, ghost_coords], dim=0).float()\n",
    "        \n",
    "        # Create latent queries grid over the per-batch bounding box\n",
    "        coords_min = torch.min(point_coords, dim=0).values\n",
    "        coords_max = torch.max(point_coords, dim=0).values\n",
    "        latent_query_arr = [\n",
    "            torch.linspace(coords_min[i], coords_max[i], latent_query_dims[i], device=device)\n",
    "            for i in range(coord_dim)\n",
    "        ]\n",
    "        latent_queries = torch.stack(torch.meshgrid(*latent_query_arr, indexing='ij'), dim=-1)\n",
    "        \n",
    "        # Build batched sequences\n",
    "        x_list, y_list = [], []\n",
    "        for sample in batch_samples:\n",
    "            sample_input = torch.concat([sample['core_in'], sample['ghost_in']], dim=0).float().unsqueeze(0)\n",
    "            sample_output = torch.concat([sample['core_out'], sample['ghost_out']], dim=0).float().unsqueeze(0)\n",
    "            x_list.append(sample_input)\n",
    "            y_list.append(sample_output)\n",
    "        \n",
    "        x = torch.cat(x_list, dim=0)  # [B, N_points, input_window_size * n_target_cols]\n",
    "        y = torch.cat(y_list, dim=0)  # [B, N_points, output_window_size * n_target_cols]\n",
    "        \n",
    "        return {\n",
    "            'point_coords': point_coords,\n",
    "            'latent_queries': latent_queries,\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'core_len': len(core_coords),\n",
    "            'patch_id': sample['patch_id']\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "# Create sampler and dataloader\n",
    "sampler = PatchBatchSampler(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle_within_batches=True,\n",
    "    shuffle_patches=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "collate_fn = make_collate_fn()\n",
    "train_loader = DataLoader(train_ds, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76aae1",
   "metadata": {},
   "source": [
    "## 5. Load GINO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22917bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /srv/scratch/z5370003/projects/results/04_groundwater/variable_density/GINO/multi_col/mass_conc_head/exp_lr4.8e4_exp_bs512/gino_multi_20251010_092951/checkpoints/checkpoint_epoch_0009.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pbs.7183938.kman.restech.unsw.edu.au/ipykernel_2685386/3554786980.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model configuration:\n",
      "- FNO modes: (12, 12, 8)\n",
      "- FNO layers: 4\n",
      "- Hidden channels: 64\n",
      "- GNO radius: 0.15\n",
      "- Latent dims: (32, 32, 24)\n",
      "- Target columns: ['mass_concentration', 'head']\n",
      "- Number of target columns: 2\n",
      "\n",
      "Model loaded successfully!\n",
      "Epoch: 10\n",
      "Train loss: 0.330761\n",
      "Val loss: 0.339951\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from: {model_path}\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Extract saved configuration\n",
    "saved_args = checkpoint['args']\n",
    "print(\"\\nSaved model configuration:\")\n",
    "print(f\"- FNO modes: {saved_args.fno_n_modes}\")\n",
    "print(f\"- FNO layers: {saved_args.fno_n_layers}\")\n",
    "print(f\"- Hidden channels: {saved_args.fno_hidden_channels}\")\n",
    "print(f\"- GNO radius: {saved_args.gno_radius}\")\n",
    "print(f\"- Latent dims: {saved_args.latent_query_dims}\")\n",
    "print(f\"- Target columns: {saved_args.target_cols}\")\n",
    "print(f\"- Number of target columns: {saved_args.n_target_cols}\")\n",
    "\n",
    "# Initialize model with saved configuration\n",
    "model = GINO(\n",
    "    # Input GNO configuration\n",
    "    in_gno_coord_dim=saved_args.coord_dim,\n",
    "    in_gno_radius=saved_args.gno_radius,\n",
    "    in_gno_out_channels=saved_args.in_gno_out_channels,\n",
    "    in_gno_channel_mlp_layers=saved_args.in_gno_channel_mlp_layers,\n",
    "    \n",
    "    # FNO configuration\n",
    "    fno_n_layers=saved_args.fno_n_layers,\n",
    "    fno_n_modes=saved_args.fno_n_modes,\n",
    "    fno_hidden_channels=saved_args.fno_hidden_channels,\n",
    "    lifting_channels=saved_args.lifting_channels,\n",
    "    \n",
    "    # Output GNO configuration\n",
    "    out_gno_coord_dim=saved_args.coord_dim,\n",
    "    out_gno_radius=saved_args.gno_radius,\n",
    "    out_gno_channel_mlp_layers=saved_args.out_gno_channel_mlp_layers,\n",
    "    projection_channel_ratio=saved_args.projection_channel_ratio,\n",
    "    out_channels=saved_args.out_channels,\n",
    ").to(device)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Epoch: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Train loss: {checkpoint['train_losses'][-1]:.6f}\")\n",
    "print(f\"Val loss: {checkpoint['val_losses'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c70e23",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "687c7128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes:\n",
      "  point_coords: torch.Size([512, 3])\n",
      "  latent_queries: torch.Size([32, 32, 24, 3])\n",
      "  x (input): torch.Size([4, 512, 10])\n",
      "  y (target): torch.Size([4, 512, 10])\n",
      "  core_len: 405\n",
      "\n",
      "Output shape: torch.Size([4, 512, 10])\n",
      "Expected shape: [batch_size=4, n_points=512, output_window_size * n_target_cols=10]\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Move to device\n",
    "point_coords = batch['point_coords'].to(device).float()\n",
    "latent_queries = batch['latent_queries'].to(device).float()\n",
    "x = batch['x'].to(device).float()\n",
    "y = batch['y'].to(device).float()\n",
    "core_len = batch['core_len']\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  point_coords: {point_coords.shape}\")\n",
    "print(f\"  latent_queries: {latent_queries.shape}\")\n",
    "print(f\"  x (input): {x.shape}\")\n",
    "print(f\"  y (target): {y.shape}\")\n",
    "print(f\"  core_len: {core_len}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_geom=point_coords,\n",
    "        latent_queries=latent_queries,\n",
    "        x=x,\n",
    "        output_queries=point_coords,\n",
    "    )\n",
    "\n",
    "print(f\"\\nOutput shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size={x.shape[0]}, n_points={point_coords.shape[0]}, output_window_size * n_target_cols={output_window_size * n_target_cols}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1efc76",
   "metadata": {},
   "source": [
    "## 7. Define Custom Loss Function\n",
    "\n",
    "This custom loss function allows:\n",
    "- Different loss weights for different target variables\n",
    "- Different loss functions per variable (e.g., MSE for mass_concentration, MAE for head)\n",
    "- Reshaping predictions from concatenated format to [batch, points, timesteps, n_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c6897-debc-4dd5-be38-385b548d5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.neuralop.losses import LpLoss\n",
    "\n",
    "def variance_aware_multicol_loss(\n",
    "    y_pred,\n",
    "    y_true,\n",
    "    output_window_size,\n",
    "    target_cols,\n",
    "    lambda_conc_focus=0.5,\n",
    "    alpha=0.3,\n",
    "    beta=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    y_pred, y_true: [B, N_points, T_out * C]\n",
    "    output_window_size: T_out\n",
    "    target_cols: list like ['mass_concentration', 'head']\n",
    "    lambda_conc_focus: how much extra weight to put on variance-aware conc loss\n",
    "    alpha: base weight for low-variance nodes (0<alpha<1)\n",
    "    beta: exponent controlling how sharply we emphasise high-variance nodes\n",
    "    \"\"\"\n",
    "\n",
    "    B, N, TC = y_pred.shape\n",
    "    C = TC // output_window_size\n",
    "    assert TC == output_window_size * C\n",
    "\n",
    "    # reshape to [B, N, T_out, C]\n",
    "    y_pred = y_pred.view(B, N, output_window_size, C)\n",
    "    y_true = y_true.view(B, N, output_window_size, C)\n",
    "\n",
    "    # Loss function\n",
    "    global_loss_fn = LpLoss(d=2, p=2, reduce_dims=[0, 1], reductions='mean')\n",
    "    local_loss_fn = LpLoss(d=1, p=2, reductions='mean')\n",
    "\n",
    "    # ----- 1) global MSE over all variables -----\n",
    "    global_loss = global_loss_fn(y_pred, y_true)\n",
    "\n",
    "    # ----- 2) variance-aware term for concentration -----\n",
    "    conc_idx = target_cols.index('mass_concentration')  # assumes name present\n",
    "\n",
    "    conc_pred = y_pred[..., conc_idx]   # [B, N, T]\n",
    "    conc_true = y_true[..., conc_idx]   # [B, N, T]\n",
    "\n",
    "    # node-wise temporal variance (on *normalized* targets)\n",
    "    with torch.no_grad():\n",
    "        # var over time dimension\n",
    "        var_t = conc_true.var(dim=[0, 2], unbiased=False)  # [N]\n",
    "        \n",
    "        # normalise variance within the batch\n",
    "        # (avoid division by tiny mean)\n",
    "        var_norm = var_t / (var_t.mean() + 1e-6)\n",
    "\n",
    "        # map to weights in [alpha, ~1] with emphasis on high variance\n",
    "        #   w = alpha + (1-alpha) * var_norm^beta, then renormalise mean to 1\n",
    "        weights = alpha + (1.0 - alpha) * (var_norm ** beta)\n",
    "        weights = weights / (weights.mean() + 1e-6)   # keep gradients stable\n",
    "        \n",
    "\n",
    "    conc_l2 = local_loss_fn(conc_pred, conc_true)        # [N]\n",
    "    conc_var_loss = (weights * conc_l2).mean()\n",
    "\n",
    "    # ----- 3) combine -----\n",
    "    loss = (1.0 - lambda_conc_focus) * global_loss + lambda_conc_focus * conc_var_loss\n",
    "\n",
    "    return loss, global_loss, conc_var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0457d-50c1-40c9-a101-4106381bdb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "382b40cb-1ae9-480a-ac0e-8f91dfca9d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes:\n",
      "  point_coords: torch.Size([512, 3])\n",
      "  latent_queries: torch.Size([32, 32, 24, 3])\n",
      "  x (input): torch.Size([4, 512, 10])\n",
      "  y (target): torch.Size([4, 512, 10])\n",
      "  core_len: 405\n",
      "tensor(512., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8502, device='cuda:0'),\n",
       " {'global_mse': tensor(0.3516, device='cuda:0'),\n",
       "  'conc_var_loss': tensor(1.3488, device='cuda:0')})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a single batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Move to device\n",
    "point_coords = batch['point_coords'].to(device).float()\n",
    "latent_queries = batch['latent_queries'].to(device).float()\n",
    "x = batch['x'].to(device).float()\n",
    "y = batch['y'].to(device).float()\n",
    "core_len = batch['core_len']\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  point_coords: {point_coords.shape}\")\n",
    "print(f\"  latent_queries: {latent_queries.shape}\")\n",
    "print(f\"  x (input): {x.shape}\")\n",
    "print(f\"  y (target): {y.shape}\")\n",
    "print(f\"  core_len: {core_len}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_geom=point_coords,\n",
    "        latent_queries=latent_queries,\n",
    "        x=x,\n",
    "        output_queries=point_coords,\n",
    "    )\n",
    "\n",
    "    loss = variance_aware_multicol_loss(\n",
    "        y_pred=outputs,\n",
    "        y_true=y,\n",
    "        output_window_size=output_window_size,\n",
    "        target_cols=target_cols,\n",
    "    )\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ad8e4-8763-418c-a8ba-1200f5f42c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (torch-env)",
   "language": "python",
   "name": "conda_torch-env_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
