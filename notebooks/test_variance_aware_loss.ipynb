{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292b9bbc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23b3678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, '/srv/scratch/z5370003/projects/src/04_groundwater/variable_density')\n",
    "\n",
    "from src.data.transform import Normalize\n",
    "from src.data.patch_dataset_multi_col import GWPatchDatasetMultiCol\n",
    "from src.data.batch_sampler import PatchBatchSampler\n",
    "from src.models.neuralop.gino import GINO\n",
    "from src.models.neuralop.losses import LpLoss, H1Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393b06c",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Using the same configuration as the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fae14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Target columns: ['mass_concentration', 'head']\n",
      "Input window: 5, Output window: 1\n"
     ]
    }
   ],
   "source": [
    "# Create configuration object\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Data directories\n",
    "        self.base_data_dir = '/srv/scratch/z5370003/projects/data/groundwater/FEFLOW/coastal/variable_density/'\n",
    "        self.raw_data_subdir = 'all'\n",
    "        self.patch_data_subdir = 'filter_patch'\n",
    "        self.raw_data_dir = os.path.join(self.base_data_dir, self.raw_data_subdir)\n",
    "        self.patch_data_dir = os.path.join(self.base_data_dir, self.patch_data_subdir)\n",
    "        \n",
    "        # Target columns\n",
    "        self.target_cols = ['mass_concentration', 'head']\n",
    "        self.target_col_indices = [0, 1]\n",
    "        self.n_target_cols = 2\n",
    "        \n",
    "        # Window sizes\n",
    "        self.input_window_size = 5\n",
    "        self.output_window_size = 1\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 512\n",
    "        self.shuffle_within_batches = True\n",
    "        self.shuffle_patches = True\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Loss parameters\n",
    "        self.lambda_conc_focus = 0.5\n",
    "        self.var_aware_alpha = 0.3\n",
    "        self.var_aware_beta = 2.0\n",
    "        \n",
    "        # Model parameters\n",
    "        self.coord_dim = 3\n",
    "        self.gno_radius = 0.15\n",
    "        self.in_gno_out_channels = 10  # input_window_size * n_target_cols = 5 * 2\n",
    "        self.in_gno_channel_mlp_layers = [32, 64, 32]\n",
    "        self.fno_n_layers = 4\n",
    "        self.fno_n_modes = (12, 12, 8)\n",
    "        self.fno_hidden_channels = 64\n",
    "        self.lifting_channels = 64\n",
    "        self.out_gno_channel_mlp_layers = [32, 64, 32]\n",
    "        self.projection_channel_ratio = 2\n",
    "        self.out_channels = 2  # output_window_size * n_target_cols = 1 * 2\n",
    "        self.latent_query_dims = (32, 32, 24)\n",
    "\n",
    "args = Config()\n",
    "print(f\"Device: {args.device}\")\n",
    "print(f\"Target columns: {args.target_cols}\")\n",
    "print(f\"Input window: {args.input_window_size}, Output window: {args.output_window_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23146f",
   "metadata": {},
   "source": [
    "## Data Transforms\n",
    "\n",
    "Calculate normalization statistics for coordinates and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad023c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinate mean: [ 3.57225665e+05  6.45774324e+06 -9.27782248e+00]\n",
      "Coordinate std: [569.1699999  566.35797379  15.26565618]\n",
      "\n",
      "Observation mean: [1.77942252e+04 3.95881156e-01 9.48469883e+01]\n",
      "Observation std: [1.55859465e+04 2.13080032e-01 1.51226320e+02]\n"
     ]
    }
   ],
   "source": [
    "# Calculate coordinate transform\n",
    "df = pd.read_csv(os.path.join(args.raw_data_dir, '0000.csv'))\n",
    "coord_mean = df[['X', 'Y', 'Z']].mean().values\n",
    "coord_std = df[['X', 'Y', 'Z']].std().values\n",
    "coord_transform = Normalize(mean=coord_mean, std=coord_std)\n",
    "\n",
    "print(f\"Coordinate mean: {coord_mean}\")\n",
    "print(f\"Coordinate std: {coord_std}\")\n",
    "\n",
    "# Calculate observation transform\n",
    "obs_cols = ['mass_concentration', 'head', 'pressure']\n",
    "obs_mean = df[obs_cols].mean().values\n",
    "obs_std = df[obs_cols].std().values\n",
    "obs_transform = Normalize(mean=obs_mean, std=obs_std)\n",
    "\n",
    "print(f\"\\nObservation mean: {obs_mean}\")\n",
    "print(f\"Observation std: {obs_std}\")\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d037802",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Create training dataset with the configured transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0250b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 13260 sequences\n",
      "Building patch_ids cache...\n",
      "Cached 13260 patch_ids\n",
      "Number of patches: 20\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "train_ds = GWPatchDatasetMultiCol(\n",
    "    data_path=args.patch_data_dir,\n",
    "    dataset='train',\n",
    "    coord_transform=coord_transform,\n",
    "    obs_transform=obs_transform,\n",
    "    input_window_size=args.input_window_size,\n",
    "    output_window_size=args.output_window_size,\n",
    "    target_col_indices=args.target_col_indices,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_ds)} sequences\")\n",
    "\n",
    "# Get unique patch IDs\n",
    "patch_ids = train_ds.get_all_patch_ids()\n",
    "unique_patches = len(np.unique(patch_ids))\n",
    "print(f\"Number of patches: {unique_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a26b60",
   "metadata": {},
   "source": [
    "## Create DataLoader with Collate Function\n",
    "\n",
    "Set up the batch sampler and collate function to create batches from the same patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d439f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building patch groups (one-time operation)...\n",
      "Found 20 patches with 13260 total samples\n",
      "Patch sizes: min=663, max=663, avg=663.0\n",
      "Pre-built 40 batches\n",
      "DataLoader created with 40 batches\n"
     ]
    }
   ],
   "source": [
    "def create_collate_fn(args):\n",
    "    \"\"\"Create collate function that batches samples from the same patch.\"\"\"\n",
    "    def collate_fn(batch_samples):\n",
    "        \"\"\"Collate function that combines samples into a batch.\n",
    "        \n",
    "        Args:\n",
    "            batch_samples: List of sample dictionaries from the same patch\n",
    "            \n",
    "        Returns:\n",
    "            dict: Batch dictionary with combined point cloud and sequences\n",
    "        \"\"\"\n",
    "        # All samples in the batch come from the same patch (by sampler design)\n",
    "        core_coords = batch_samples[0]['core_coords']\n",
    "        ghost_coords = batch_samples[0]['ghost_coords']\n",
    "\n",
    "        # Single point cloud per batch: [N_core+N_ghost, 3]\n",
    "        # Concatenate core and ghost points to form complete spatial domain\n",
    "        point_coords = torch.concat([core_coords, ghost_coords], dim=0).float().to(args.device)\n",
    "\n",
    "        # Create latent queries grid over the per-batch bounding box\n",
    "        # This provides a regular grid for the FNO component\n",
    "        coords_min = torch.min(point_coords, dim=0).values\n",
    "        coords_max = torch.max(point_coords, dim=0).values\n",
    "        latent_query_arr = [\n",
    "            torch.linspace(coords_min[i], coords_max[i], args.latent_query_dims[i], device=args.device)\n",
    "            for i in range(args.coord_dim)\n",
    "        ]\n",
    "        # Create meshgrid and stack to get [Qx, Qy, Qz, 3] tensor\n",
    "        latent_queries = torch.stack(torch.meshgrid(*latent_query_arr, indexing='ij'), dim=-1)\n",
    "\n",
    "        # Build batched sequences: concat along points (dim=0), batch along dim=0\n",
    "        x_list, y_list = [], []\n",
    "        for sample in batch_samples:\n",
    "            # Combine core and ghost inputs/outputs for each sample\n",
    "            # Note: sequences are already concatenated across target columns in the dataset\n",
    "            sample_input = torch.concat([sample['core_in'], sample['ghost_in']], dim=0).float().unsqueeze(0)\n",
    "            sample_output = torch.concat([sample['core_out'], sample['ghost_out']], dim=0).float().unsqueeze(0)\n",
    "            x_list.append(sample_input)\n",
    "            y_list.append(sample_output)\n",
    "\n",
    "        # Stack all sequences into batch tensors and move to device\n",
    "        x = torch.cat(x_list, dim=0).to(args.device)  # [B, N_points, input_window_size * n_target_cols]\n",
    "        y = torch.cat(y_list, dim=0).to(args.device)  # [B, N_points, output_window_size * n_target_cols]\n",
    "\n",
    "        # Return batch dictionary\n",
    "        batch = {\n",
    "            'point_coords': point_coords,      # [N_points, 3] - already on device\n",
    "            'latent_queries': latent_queries,  # [Qx, Qy, Qz, 3] - already on device\n",
    "            'x': x,                           # [B, N_points, input_window_size * n_target_cols] - moved to device\n",
    "            'y': y,                           # [B, N_points, output_window_size * n_target_cols] - moved to device\n",
    "            'core_len': len(core_coords),     # Number of core points (for loss masking)\n",
    "            'patch_id': batch_samples[0]['patch_id'],  # For reference\n",
    "        }\n",
    "        return batch\n",
    "    return collate_fn\n",
    "\n",
    "# Create batch sampler and data loader\n",
    "batch_sampler = PatchBatchSampler(\n",
    "    dataset=train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle_within_batches=args.shuffle_within_batches,\n",
    "    shuffle_patches=args.shuffle_patches,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_sampler=batch_sampler,\n",
    "    collate_fn=create_collate_fn(args),\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9044bd6",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "Create the GINO model with the configured parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8e1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on cuda\n",
      "Total parameters: 23,658,508\n"
     ]
    }
   ],
   "source": [
    "model = GINO(\n",
    "    # Input GNO configuration\n",
    "    in_gno_coord_dim=args.coord_dim,\n",
    "    in_gno_radius=args.gno_radius,\n",
    "    in_gno_out_channels=args.in_gno_out_channels,\n",
    "    in_gno_channel_mlp_layers=args.in_gno_channel_mlp_layers,\n",
    "    \n",
    "    # FNO configuration\n",
    "    fno_n_layers=args.fno_n_layers,\n",
    "    fno_n_modes=args.fno_n_modes,\n",
    "    fno_hidden_channels=args.fno_hidden_channels,\n",
    "    lifting_channels=args.lifting_channels,\n",
    "    \n",
    "    # Output GNO configuration\n",
    "    out_gno_coord_dim=args.coord_dim,\n",
    "    out_gno_radius=args.gno_radius,\n",
    "    out_gno_channel_mlp_layers=args.out_gno_channel_mlp_layers,\n",
    "    projection_channel_ratio=args.projection_channel_ratio,\n",
    "    out_channels=args.out_channels,\n",
    "    \n",
    "    # Neighbor search settings\n",
    "    use_open3d_neighbor_search=True,\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model initialized on {args.device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa4572",
   "metadata": {},
   "source": [
    "## Get a Batch\n",
    "\n",
    "Extract one batch from the training data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44119d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch contents:\n",
      "  point_coords shape: torch.Size([512, 3])\n",
      "  latent_queries shape: torch.Size([32, 32, 24, 3])\n",
      "  x (inputs) shape: torch.Size([512, 512, 10])\n",
      "  y (targets) shape: torch.Size([512, 512, 2])\n",
      "  core_len (core points): 405\n",
      "  patch_id: 1\n"
     ]
    }
   ],
   "source": [
    "# Get first batch\n",
    "# batch_iter = iter(train_loader)\n",
    "batch = next(batch_iter)\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "print(f\"  point_coords shape: {batch['point_coords'].shape}\")\n",
    "print(f\"  latent_queries shape: {batch['latent_queries'].shape}\")\n",
    "print(f\"  x (inputs) shape: {batch['x'].shape}\")\n",
    "print(f\"  y (targets) shape: {batch['y'].shape}\")\n",
    "print(f\"  core_len (core points): {batch['core_len']}\")\n",
    "print(f\"  patch_id: {batch['patch_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356a726",
   "metadata": {},
   "source": [
    "## Variance-Aware Loss Function\n",
    "\n",
    "Define the variance-aware multi-column loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7c7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_aware_multicol_loss(\n",
    "    y_pred,\n",
    "    y_true,\n",
    "    output_window_size,\n",
    "    target_cols,\n",
    "    lambda_conc_focus=0.5,\n",
    "    alpha=0.3,\n",
    "    beta=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    y_pred, y_true: [B, N_points, T_out * C]\n",
    "    output_window_size: T_out\n",
    "    target_cols: list like ['mass_concentration', 'head']\n",
    "    lambda_conc_focus: how much extra weight to put on variance-aware conc loss\n",
    "    alpha: base weight for low-variance nodes (0<alpha<1)\n",
    "    beta: exponent controlling how sharply we emphasise high-variance nodes\n",
    "    \"\"\"\n",
    "\n",
    "    B, N, TC = y_pred.shape\n",
    "    C = TC // output_window_size\n",
    "    assert TC == output_window_size * C\n",
    "\n",
    "    # reshape to [B, N, T_out, C]\n",
    "    y_pred = y_pred.view(B, N, output_window_size, C)\n",
    "    y_true = y_true.view(B, N, output_window_size, C)\n",
    "\n",
    "    # Loss function\n",
    "    global_loss_fn = LpLoss(d=2, p=2, reduce_dims=[0, 1], reductions='mean')\n",
    "    local_loss_fn = LpLoss(d=1, p=2, reductions='mean')\n",
    "\n",
    "    # ----- 1) global MSE over all variables -----\n",
    "    global_loss = global_loss_fn(y_pred, y_true)\n",
    "\n",
    "    # ----- 2) variance-aware term for concentration -----\n",
    "    conc_idx = target_cols.index('mass_concentration')  # assumes name present\n",
    "\n",
    "    conc_pred = y_pred[..., conc_idx]   # [B, N, T]\n",
    "    conc_true = y_true[..., conc_idx]   # [B, N, T]\n",
    "\n",
    "    # node-wise temporal variance (on *normalized* targets)\n",
    "    with torch.no_grad():\n",
    "        # var over time dimension\n",
    "        var_t = conc_true.var(dim=[0, 2], unbiased=False)  # [N]\n",
    "        \n",
    "        # normalise variance within the batch\n",
    "        # (avoid division by tiny mean)\n",
    "        var_norm = var_t / (var_t.mean() + 1e-6)\n",
    "\n",
    "        # map to weights in [alpha, ~1] with emphasis on high variance\n",
    "        #   w = alpha + (1-alpha) * var_norm^beta, then renormalise mean to 1\n",
    "        weights = alpha + (1.0 - alpha) * (var_norm ** beta)\n",
    "        weights = weights / (weights.mean() + 1e-6)   # keep gradients stable\n",
    "        \n",
    "\n",
    "    conc_l2 = local_loss_fn(conc_pred, conc_true)        # [N]\n",
    "    conc_var_loss = (weights * conc_l2).mean()\n",
    "\n",
    "    # ----- 3) combine -----\n",
    "    loss = (1.0 - lambda_conc_focus) * global_loss + lambda_conc_focus * conc_var_loss\n",
    "\n",
    "    return {'loss_total': loss, \n",
    "            'loss_global': global_loss.detach(),\n",
    "            'loss_conc_var': conc_var_loss.detach()}, var_t, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8eea0d",
   "metadata": {},
   "source": [
    "## Test Forward Pass and Loss Computation\n",
    "\n",
    "Run a forward pass and compute the loss on the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f10317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output shape: torch.Size([512, 512, 2])\n",
      "Expected shape: [batch_size=512, n_points=512, out_channels=2]\n",
      "\n",
      "Core predictions shape: torch.Size([512, 405, 2])\n",
      "Core targets shape: torch.Size([512, 405, 2])\n",
      "Number of core points: 405\n",
      "\n",
      "============================================================\n",
      "Loss Components:\n",
      "============================================================\n",
      "Total Loss:        1.090888\n",
      "Global Loss:       1.039078\n",
      "Conc Variance Loss: 1.142697\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        input_geom=batch['point_coords'],\n",
    "        latent_queries=batch['latent_queries'],\n",
    "        x=batch['x'],\n",
    "        output_queries=batch['point_coords'],\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel output shape: {outputs.shape}\")\n",
    "    print(f\"Expected shape: [batch_size={args.batch_size}, n_points={batch['point_coords'].shape[0]}, out_channels={args.out_channels}]\")\n",
    "    \n",
    "    # Extract core points for loss computation (first core_len points are core points)\n",
    "    core_len = batch['core_len']\n",
    "    y_pred_core = outputs[:, :core_len, :]  # [B, N_core, out_channels]\n",
    "    y_true_core = batch['y'][:, :core_len, :]  # [B, N_core, out_channels]\n",
    "    \n",
    "    print(f\"\\nCore predictions shape: {y_pred_core.shape}\")\n",
    "    print(f\"Core targets shape: {y_true_core.shape}\")\n",
    "    print(f\"Number of core points: {core_len}\")\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_dict, var_t, weights = variance_aware_multicol_loss(\n",
    "        y_pred_core,\n",
    "        y_true_core,\n",
    "        output_window_size=args.output_window_size,\n",
    "        target_cols=args.target_cols,\n",
    "        lambda_conc_focus=args.lambda_conc_focus,\n",
    "        alpha=args.var_aware_alpha,\n",
    "        beta=args.var_aware_beta,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loss Components:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Loss:        {loss_dict['loss_total'].item():.6f}\")\n",
    "    print(f\"Global Loss:       {loss_dict['loss_global'].item():.6f}\")\n",
    "    print(f\"Conc Variance Loss: {loss_dict['loss_conc_var'].item():.6f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc3802-25bd-423a-bfbc-ebc83c382988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f79e9e3-e069-46c1-9693-7c9c238688f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e225af-f45f-4620-8195-35cb4d36b702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x151d7067f8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(var_t.cpu().numpy(), weights.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22625ce-636b-4652-979d-0792aaaa56b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de0550-f65e-4c67-b842-7ef8a2794ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59b065-e802-4308-8888-90a6a25e88b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd158149-1300-4dcd-bf73-14cc04ddc11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb3c381e",
   "metadata": {},
   "source": [
    "## Analyze Loss Components\n",
    "\n",
    "Examine how the loss is distributed across variables and timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Reshape predictions and targets\n",
    "    B, N, D = y_pred_core.shape\n",
    "    n_cols = len(args.target_cols)\n",
    "    \n",
    "    y_pred_reshaped = y_pred_core.reshape(B, N, args.output_window_size, n_cols)\n",
    "    y_true_reshaped = y_true_core.reshape(B, N, args.output_window_size, n_cols)\n",
    "    \n",
    "    # Compute MSE per column\n",
    "    mse_per_col = ((y_pred_reshaped - y_true_reshaped) ** 2).mean(dim=(0, 1, 2))  # [n_cols]\n",
    "    \n",
    "    print(\"\\nMSE per target column:\")\n",
    "    for i, col_name in enumerate(args.target_cols):\n",
    "        print(f\"  {col_name}: {mse_per_col[i].item():.6f}\")\n",
    "    \n",
    "    # Analyze concentration variance if available\n",
    "    conc_idx = None\n",
    "    for idx, col in enumerate(args.target_cols):\n",
    "        if 'mass_concentration' in col or 'concentration' in col:\n",
    "            conc_idx = idx\n",
    "            break\n",
    "    \n",
    "    if conc_idx is not None:\n",
    "        conc_true = y_true_reshaped[:, :, :, conc_idx]  # [B, N, T]\n",
    "        spatial_var = conc_true.var(dim=1, unbiased=False)  # [B, T]\n",
    "        \n",
    "        print(f\"\\nConcentration spatial variance per timestep:\")\n",
    "        for t in range(args.output_window_size):\n",
    "            avg_var = spatial_var[:, t].mean().item()\n",
    "            print(f\"  Timestep {t}: {avg_var:.6f}\")\n",
    "        \n",
    "        # Compute weights\n",
    "        eps = 1e-6\n",
    "        weight = 1.0 / (args.var_aware_alpha * spatial_var + eps) ** args.var_aware_beta\n",
    "        weight_normalized = weight / weight.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        print(f\"\\nNormalized variance-aware weights per timestep:\")\n",
    "        for t in range(args.output_window_size):\n",
    "            avg_weight = weight_normalized[:, t].mean().item()\n",
    "            print(f\"  Timestep {t}: {avg_weight:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b414c0",
   "metadata": {},
   "source": [
    "## Visualize Predictions vs Targets\n",
    "\n",
    "Plot predictions and targets for the first sample in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first sample\n",
    "sample_idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_sample = y_pred_core[sample_idx].cpu().numpy()  # [N_core, out_channels]\n",
    "    y_true_sample = y_true_core[sample_idx].cpu().numpy()  # [N_core, out_channels]\n",
    "    \n",
    "    # Reshape to separate columns\n",
    "    y_pred_sample = y_pred_sample.reshape(-1, args.output_window_size, n_cols)\n",
    "    y_true_sample = y_true_sample.reshape(-1, args.output_window_size, n_cols)\n",
    "    \n",
    "    # Create subplots for each column\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(6*n_cols, 5))\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for col_idx, col_name in enumerate(args.target_cols):\n",
    "        ax = axes[col_idx]\n",
    "        \n",
    "        # Get values for this column (first timestep)\n",
    "        pred_vals = y_pred_sample[:, 0, col_idx]\n",
    "        true_vals = y_true_sample[:, 0, col_idx]\n",
    "        \n",
    "        # Scatter plot: predictions vs targets\n",
    "        ax.scatter(true_vals, pred_vals, alpha=0.5, s=1)\n",
    "        \n",
    "        # Add diagonal line (perfect prediction)\n",
    "        min_val = min(true_vals.min(), pred_vals.min())\n",
    "        max_val = max(true_vals.max(), pred_vals.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "        \n",
    "        ax.set_xlabel('True values')\n",
    "        ax.set_ylabel('Predicted values')\n",
    "        ax.set_title(f'{col_name}\\n(Sample {sample_idx}, Timestep 0)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nPrediction Statistics (Sample 0, Timestep 0):\")\n",
    "    for col_idx, col_name in enumerate(args.target_cols):\n",
    "        pred_vals = y_pred_sample[:, 0, col_idx]\n",
    "        true_vals = y_true_sample[:, 0, col_idx]\n",
    "        \n",
    "        mse = np.mean((pred_vals - true_vals) ** 2)\n",
    "        mae = np.mean(np.abs(pred_vals - true_vals))\n",
    "        \n",
    "        print(f\"\\n{col_name}:\")\n",
    "        print(f\"  True range: [{true_vals.min():.4f}, {true_vals.max():.4f}]\")\n",
    "        print(f\"  Pred range: [{pred_vals.min():.4f}, {pred_vals.max():.4f}]\")\n",
    "        print(f\"  MSE: {mse:.6f}\")\n",
    "        print(f\"  MAE: {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c50e1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. Model initialization with the exact training configuration\n",
    "2. A batch of data ready for testing\n",
    "3. Forward pass computation\n",
    "4. Variance-aware loss computation with component breakdown\n",
    "5. Analysis tools to examine how the loss affects different variables and timesteps\n",
    "\n",
    "You can now experiment with different loss parameters:\n",
    "- `lambda_conc_focus`: Balance between global and variance-aware loss (0-1)\n",
    "- `var_aware_alpha`: Scaling factor for variance weighting\n",
    "- `var_aware_beta`: Exponent for variance weighting (higher = more aggressive weighting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (torch-env)",
   "language": "python",
   "name": "conda_torch-env_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
